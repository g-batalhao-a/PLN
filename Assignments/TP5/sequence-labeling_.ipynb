{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt), based on [Named Entity Recognition with NLTK and SpaCy](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da) by Susan Li.\n",
    "\n",
    "# SEQUENCE LABELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Tagging and Named Entity Recognition using NLTK or spaCy\n",
    "\n",
    "Part Of Speech (POS) tagging and Named Entity Recognition (NER) are the two most well known examples of sequence labeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "POS tagging consists of assigning to each word its morpho-syntactic category.\n",
    "\n",
    "NLTK includes a [POS tagger](https://www.nltk.org/api/nltk.tag.html) that we can use. We can check the tagset used by the tagger as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the POS tagger, we first need to tokenize the text. Try out NLTK's [*pos_tag*](https://www.nltk.org/api/nltk.tag.html) with the following text, and analyse the POS tags you get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"\"\"European authorities fined Google a record $5.1 billion on Wednesday \n",
    "for abusing its power in the mobile phone market and \n",
    "ordered the company to alter its practices.\"\"\"\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a POS tagger in NLTK\n",
    "\n",
    "NLTK also allows you to train simple tagging models based on n-grams, where the model takes into account the tags assigned to the *n-1* words preceding the target word.\n",
    "\n",
    "Let's try that using [Floresta Sint√°(c)tica](https://www.linguateca.pt/Floresta/), a Portuguese corpus annotated with POS tags (we will follow [this tuturial](https://www.nltk.org/howto/portuguese_en.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta\n",
    "\n",
    "print(len(floresta.sents()))\n",
    "print(floresta.sents())\n",
    "print(floresta.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags consist of some syntactic information, followed by a plus sign, followed by a conventional part-of-speech tag. We need to strip off the material before the plus sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_tag(t):\n",
    "    if \"+\" in t:\n",
    "        return t[t.index(\"+\")+1:]\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in floresta.tagged_sents()]\n",
    "tsents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split our data into a train and a test set. Let's keep 100 sentences in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tsents[100:]\n",
    "test = tsents[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we do with an unigram tagger, which simply assigns the most likely tag for any given token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger1 = nltk.UnigramTagger(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how the tagger performs on the test set by using the *evaluate* method, which gives us the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger1.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try tagging a user-generated sentence. Don't forget to tokenize it and lower-case the obtained tokens, following what we have done with the corpus above. To tag a list of tokens, you can invoke the *tag* method on the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try out a bigram model for POS tagging, which will take into account the tag assigned to the previous word. If that previous word hasn't been seen in the training set, however, the model will fail to tag the target word, even if it did appear in the training set. For that reason, it is convenient to backoff to the previous unigram tagger -- if we know nothing about the tag of the previous word, we can still use the most likely tag for the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *verbose* flag outputs some information, namely the amount of backoff used.\n",
    "\n",
    "Check the performance of this tagger, and compare it with the performance of a bigram tagger with no backoff strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a trigram tagger with backoff to the bigram tagger and check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "NER consists of detecting named entities in the text, which can correspond to several different categories, such as person names, organizations, dates, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n",
    "Our first attempt to detect names in English may consist of chunking certain parts of the text that correspond to a pattern of POS tags. For that, we define a pattern consisting of (i) an optional *determinant*, optionally followed by (ii) *adjectives*, followed by (iii) a *noun*.\n",
    "\n",
    "We can use NLTK's *RegexpParser* and supply it with an appropriate regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a chunk parser\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our chunk parser, we can parse our sentence's POS-tagged list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"\"\"European authorities fined Google a record $5.1 billion on Wednesday \n",
    "for abusing its power in the mobile phone market and \n",
    "ordered the company to alter its practices.\"\"\"\n",
    "pos_tokens = pos_tag(word_tokenize(text))\n",
    "\n",
    "# generating a parse tree\n",
    "cs = cp.parse(pos_tokens)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more appealing way of visualizing the result is to simply show the obtained parse tree, with *S* (for sentence) at the first level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the obtained chunks, we can generate IOB tags for each of the elements in the sentence. For each chunk, we will get a **B**egin tag for its first token, optionally followed by **I**nside tags for subsequent tokens in the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating IOB tags for the tree: one token per line, each with its POS tag and its named entity tag\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides a classifier that has already been trained to recognize named entities: [*ne_chunk*](https://www.nltk.org/book/ch07.html#duck_typing_index_term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "ne_tree = ne_chunk(pos_tokens)\n",
    "print(ne_tree)\n",
    "ne_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not the most perfect thing, is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "SpaCy includes several [language processing pipelines](https://spacy.io/usage/processing-pipelines) that streamline several NLP tasks at once. We can use one of the available [trained pipelines](https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity level\n",
    "\n",
    "SpaCy‚Äôs named entity recognition has been trained on the [OntoNotes 5](https://catalog.ldc.upenn.edu/LDC2013T19) corpus.\n",
    "We can directly obtain the entities identified by spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "doc = nlp(\"\"\"European authorities fined Google a record $5.1 billion on Wednesday \n",
    "for abusing its power in the mobile phone market and \n",
    "ordered the company to alter its practices.\"\"\")\n",
    "\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token level\n",
    "We can also get the BIO encoding for the identified entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER from a document\n",
    "Let's use spaCy to do NER on an actual web document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def url_to_string(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "\n",
    "url = 'https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news'\n",
    "clean_text = url_to_string(url)\n",
    "article = nlp(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many entities were extracted from the document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many instances are there for each entity type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the most mentioned entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out a specific sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x for x in article.sents]\n",
    "a_sentence = sentences[20]\n",
    "a_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the BIO encoding for the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x, x.pos_, x.ent_iob_, x.ent_type_) for x in a_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply output the mentioned entities and their categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict([(str(x), x.label_) for x in a_sentence.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use spaCy's [visualizers](https://spacy.io/usage/visualizers) to better show the output of the NER model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(a_sentence, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The displaCy visualizer also gets us POS information and dependency parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(a_sentence, style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting entities for the full document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    displacy.render(sent, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER for other languages\n",
    "\n",
    "Try out other spaCy [pipelines](https://spacy.io/models) for other languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
