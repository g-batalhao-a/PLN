{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt), based on [Word2Vec Tutorial Notebook](https://github.com/kavgan/nlp-in-practice/tree/master/word2vec) by Kavita Ganesan and on [Gensim's documentation on the Word2Vec Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html).\n",
    "\n",
    "# WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in Gensim\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) is a model for training word embeddings that revolutionized the way words are represented. [Gensim](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html) provides an implementation of the algorithm, with which we can train our own word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data\n",
    "\n",
    "Training embeddings requires a big corpus, the bigger the better.\n",
    "\n",
    "For illustration purposes, we'll make use of the (not very big) [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset, which includes full reviews of cars and hotels. More specifically, we'll use an 84MB compressed file with 255404 hotel reviews.\n",
    "\n",
    "This is how each review looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the whole dataset into a list, while providing some logging information.\n",
    "\n",
    "In the process of reading the data directly from the compressed file, we'll perform some pre-processing of the reviews using [gensim.utils.simple_preprocess](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html). This does some basic pre-processing such as tokenization and lowercasing, and returns back a list of tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "    logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 19:07:09,899 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
      "2022-04-12 19:07:09,900 : INFO : read 0 reviews\n",
      "2022-04-12 19:07:11,617 : INFO : read 10000 reviews\n",
      "2022-04-12 19:07:13,293 : INFO : read 20000 reviews\n",
      "2022-04-12 19:07:15,205 : INFO : read 30000 reviews\n",
      "2022-04-12 19:07:17,047 : INFO : read 40000 reviews\n",
      "2022-04-12 19:07:19,069 : INFO : read 50000 reviews\n",
      "2022-04-12 19:07:20,995 : INFO : read 60000 reviews\n",
      "2022-04-12 19:07:22,619 : INFO : read 70000 reviews\n",
      "2022-04-12 19:07:24,076 : INFO : read 80000 reviews\n",
      "2022-04-12 19:07:25,654 : INFO : read 90000 reviews\n",
      "2022-04-12 19:07:27,510 : INFO : read 100000 reviews\n",
      "2022-04-12 19:07:29,188 : INFO : read 110000 reviews\n",
      "2022-04-12 19:07:30,819 : INFO : read 120000 reviews\n",
      "2022-04-12 19:07:32,568 : INFO : read 130000 reviews\n",
      "2022-04-12 19:07:34,311 : INFO : read 140000 reviews\n",
      "2022-04-12 19:07:35,852 : INFO : read 150000 reviews\n",
      "2022-04-12 19:07:37,482 : INFO : read 160000 reviews\n",
      "2022-04-12 19:07:39,024 : INFO : read 170000 reviews\n",
      "2022-04-12 19:07:40,711 : INFO : read 180000 reviews\n",
      "2022-04-12 19:07:42,698 : INFO : read 190000 reviews\n",
      "2022-04-12 19:07:44,394 : INFO : read 200000 reviews\n",
      "2022-04-12 19:07:46,122 : INFO : read 210000 reviews\n",
      "2022-04-12 19:07:47,878 : INFO : read 220000 reviews\n"
     ]
    }
   ],
   "source": [
    "# compressed file with the data\n",
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "documents = list(read_input(data_file))\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review item becomes a list of words, so what we have is a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Word2Vec model\n",
    "\n",
    "To train a Word2Vec model, we instantiate Word2Vec and pass it the text we have loaded before. You can check the available options for [instantiation](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.Word2Vec) and for [training](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html#gensim.models.word2vec.Word2Vec.train).\n",
    "\n",
    "Training a Word2Vec model takes time, depending on your hardware. In this particular case, expect training to take something between 5 to 10 minutes on an Intel Core-i7 16GB desktop. I know, that's a very wide range, but of course it depends on which other processes are running in the same machine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "model = gensim.models.Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=10, sg=1)\n",
    "\n",
    "print(\"Training time:\", datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploiting the Word2Vec model\n",
    "\n",
    "We can now inspect the word embeddings that we have trained. We can start by looking at the embeddings of a specific word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "model.wv.get_vector(\"dirty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the words most similar to this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity\n",
    "w1 = \"dirty\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also limit to a smaller number of hits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar(positive=w1, topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar(positive=w1, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide to *most_similar* not only positive concepts, but also negative ones. This allows us to do some arithmetic on the vector representations for certain sets of words!\n",
    "\n",
    "The famous example **king - man + woman = queen** goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arithmetic: vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)\n",
    "w1 = [\"king\",'woman']\n",
    "w2 = ['man']\n",
    "model.wv.most_similar(positive=w1, negative=w2, topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the similarity scores for specific word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\", w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\", w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\", w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check which word in a list of words is an intruder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"cat\", \"dog\", \"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"bed\", \"pillow\", \"duvet\", \"shower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"car\", \"bicycle\", \"plane\", \"skate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"car\", \"bicycle\", \"bus\", \"trolley\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your own experiments! Try to find out:\n",
    "- Which word is most similar to *lift*?\n",
    "- What are the 3 words most similar to *crab*?\n",
    "- How similar are the words *waitress* and *waiter*?\n",
    "- If you take *portugal*, remove *lisbon*, and add *dublin*, what do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading a Word2Vec model\n",
    "\n",
    "You can save a trained model so that you are able to load it again in the future, and optionally continue training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full model (including trainable vectors to resume training)\n",
    "model.save(\"reviews_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full model\n",
    "model = gensim.models.Word2Vec.load(\"reviews_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading the word embeddings\n",
    "\n",
    "If you're sure you won't be training the model any longer, you can save its *KeyedVectors* (the word embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word vectors\n",
    "model.wv.save(\"reviews_wv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the embeddings, you can load them and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model word vectors\n",
    "wv = gensim.models.KeyedVectors.load(\"reviews_wv\")\n",
    "\n",
    "print(wv.most_similar(positive=\"lift\", topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Word embeddings can be visualized by reducing dimensionality of the words to 2 dimensions using [tSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "\n",
    "Given enough training data, we can observe certain patterns in the vector space, including:\n",
    "- Semantic relations: words like *cat*, *dog*, *cow*, etc. have a tendency to lie close by.\n",
    "- Syntactic relations: words like *run*, *running* or *cut*, *cutting* lie close together.\n",
    "- Arithmetic properties such as *King - Man = Queen - Woman*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def reduce_dimensions(model, num_dimensions=2, words=[]):\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    word_count = 0\n",
    "    \n",
    "    # if no word list is given, assume we want to use the whole data in the model\n",
    "    if(words == []):\n",
    "        words = model.wv.index_to_key\n",
    "\n",
    "    for word in words:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels\n",
    "\n",
    "\n",
    "# 2 dimension plotting\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels, words=[]):\n",
    "\n",
    "    random.seed(0)\n",
    "    \n",
    "    x_vals_new = np.array([])\n",
    "    y_vals_new = np.array([])\n",
    "    labels_new = np.array([])\n",
    "    if(words == []):\n",
    "        # if no word list is given, assume we want to plot the whole data\n",
    "        x_vals_new = x_vals\n",
    "        y_vals_new = y_vals\n",
    "        labels_new = labels\n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            if(labels[i] in words):\n",
    "                x_vals_new = np.append(x_vals_new,x_vals[i])\n",
    "                y_vals_new = np.append(y_vals_new,y_vals[i])\n",
    "                labels_new = np.append(labels_new,labels[i])\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals_new, y_vals_new)\n",
    "\n",
    "    # apply labels\n",
    "    for i in range(len(labels_new)):\n",
    "        plt.annotate(labels_new[i], (x_vals_new[i], y_vals_new[i]))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "words.extend([\"king\", \"man\", \"queen\", \"woman\"])\n",
    "\n",
    "vectors, labels = reduce_dimensions(model, 2, words)\n",
    "x_vals = [v[0] for v in vectors]\n",
    "y_vals = [v[1] for v in vectors]\n",
    "print(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_matplotlib(x_vals, y_vals, labels, [\"king\", \"man\", \"queen\", \"woman\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese embeddings\n",
    "\n",
    "A number of embeddings for Portuguese are available at [NILC](http://nilc.icmc.usp.br/embeddings), as well as at the [NLX-group](https://github.com/nlx-group/LX-DSemVectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# takes a while to load...\n",
    "model_pt = KeyedVectors.load_word2vec_format('skip_s100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word vectors\n",
    "model_pt.save(\"pt_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model word vectors (much faster than the above)\n",
    "model_pt = gensim.models.KeyedVectors.load(\"pt_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt.most_similar(positive=[\"cão\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt.most_similar(positive=[\"rei\", \"mulher\"], negative=[\"homem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your own experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
