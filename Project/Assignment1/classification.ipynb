{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "Firstly, we must import the dataset into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "train = pd.read_excel('OpArticles_ADUs.xlsx')\n",
    "test = pd.read_excel('OpArticles.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>node</th>\n",
       "      <th>ranges</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2516, 2556]]</td>\n",
       "      <td>O facto não é apenas fruto da ignorância</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[[2568, 2806]]</td>\n",
       "      <td>havia no seu humor mais jornalismo (mais inves...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[[3169, 3190]]</td>\n",
       "      <td>É tudo cómico na FIFA</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[[3198, 3285]]</td>\n",
       "      <td>o que todos nós permitimos que esta organizaçã...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4257, 4296]]</td>\n",
       "      <td>não nos fazem rir à custa dos poderosos</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id annotator  node          ranges  \\\n",
       "0  5d04a31b896a7fea069ef06f         A     0  [[2516, 2556]]   \n",
       "1  5d04a31b896a7fea069ef06f         A     1  [[2568, 2806]]   \n",
       "2  5d04a31b896a7fea069ef06f         A     3  [[3169, 3190]]   \n",
       "3  5d04a31b896a7fea069ef06f         A     4  [[3198, 3285]]   \n",
       "4  5d04a31b896a7fea069ef06f         A     6  [[4257, 4296]]   \n",
       "\n",
       "                                              tokens  label  \n",
       "0           O facto não é apenas fruto da ignorância  Value  \n",
       "1  havia no seu humor mais jornalismo (mais inves...  Value  \n",
       "2                              É tudo cómico na FIFA  Value  \n",
       "3  o que todos nós permitimos que esta organizaçã...  Value  \n",
       "4            não nos fazem rir à custa dos poderosos  Value  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>body</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>topics</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>url_canonical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>Pouco pão e muito circo, morte e bocejo</td>\n",
       "      <td>['José Vítor Malheiros']</td>\n",
       "      <td>O poeta espanhol António Machado escrevia, uns...</td>\n",
       "      <td>É tudo cómico na FIFA, porque todos os dias a ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Desporto', ...</td>\n",
       "      <td>2014-06-17 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/06/17/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a3fc896a7fea069f0717</td>\n",
       "      <td>Portugal nos Mundiais de Futebol de 2010 e 2014</td>\n",
       "      <td>['Rui J. Baptista']</td>\n",
       "      <td>“O mais excelente quadro posto a uma luz logo ...</td>\n",
       "      <td>Deve ser evidenciado o clima favorável criado ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Coreia do N...</td>\n",
       "      <td>2014-07-05 02:46:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/05/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a455896a7fea069f07ab</td>\n",
       "      <td>Futebol, guerra, religião</td>\n",
       "      <td>['Fernando Belo']</td>\n",
       "      <td>1. As sociedades humanas parecem ser regidas p...</td>\n",
       "      <td>O futebol parece ser um sucedâneo quer da lei ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['A guerra na Síria', 'Desporto', 'Futebol', '...</td>\n",
       "      <td>2014-07-12 16:05:33</td>\n",
       "      <td>https://www.publico.pt/2014/07/12/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a52f896a7fea069f0921</td>\n",
       "      <td>As razões do Qatar para acolher o Mundial em 2022</td>\n",
       "      <td>['Hamad bin Khalifa bin Ahmad Al Thani']</td>\n",
       "      <td>Este foi um Mundial incrível. Vimos actuações ...</td>\n",
       "      <td>Queremos cooperar plenamente com a investigaçã...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...</td>\n",
       "      <td>2014-07-27 02:00:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/27/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a8d7896a7fea069f6997</td>\n",
       "      <td>A política no campo de futebol</td>\n",
       "      <td>['Carlos Nolasco']</td>\n",
       "      <td>O futebol sempre foi um jogo aparentemente sim...</td>\n",
       "      <td>Retirar a expressão política do futebol é reti...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Albânia', 'Campeonato da Europa', 'Desporto'...</td>\n",
       "      <td>2014-10-23 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/10/23/desporto/opi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  \\\n",
       "0  5d04a31b896a7fea069ef06f   \n",
       "1  5d04a3fc896a7fea069f0717   \n",
       "2  5d04a455896a7fea069f07ab   \n",
       "3  5d04a52f896a7fea069f0921   \n",
       "4  5d04a8d7896a7fea069f6997   \n",
       "\n",
       "                                               title  \\\n",
       "0            Pouco pão e muito circo, morte e bocejo   \n",
       "1    Portugal nos Mundiais de Futebol de 2010 e 2014   \n",
       "2                          Futebol, guerra, religião   \n",
       "3  As razões do Qatar para acolher o Mundial em 2022   \n",
       "4                     A política no campo de futebol   \n",
       "\n",
       "                                    authors  \\\n",
       "0                  ['José Vítor Malheiros']   \n",
       "1                       ['Rui J. Baptista']   \n",
       "2                         ['Fernando Belo']   \n",
       "3  ['Hamad bin Khalifa bin Ahmad Al Thani']   \n",
       "4                        ['Carlos Nolasco']   \n",
       "\n",
       "                                                body  \\\n",
       "0  O poeta espanhol António Machado escrevia, uns...   \n",
       "1  “O mais excelente quadro posto a uma luz logo ...   \n",
       "2  1. As sociedades humanas parecem ser regidas p...   \n",
       "3  Este foi um Mundial incrível. Vimos actuações ...   \n",
       "4  O futebol sempre foi um jogo aparentemente sim...   \n",
       "\n",
       "                                    meta_description  topics  \\\n",
       "0  É tudo cómico na FIFA, porque todos os dias a ...  Sports   \n",
       "1  Deve ser evidenciado o clima favorável criado ...  Sports   \n",
       "2  O futebol parece ser um sucedâneo quer da lei ...  Sports   \n",
       "3  Queremos cooperar plenamente com a investigaçã...  Sports   \n",
       "4  Retirar a expressão política do futebol é reti...  Sports   \n",
       "\n",
       "                                            keywords         publish_date  \\\n",
       "0  ['Brasil', 'Campeonato do Mundo', 'Desporto', ...  2014-06-17 00:16:00   \n",
       "1  ['Brasil', 'Campeonato do Mundo', 'Coreia do N...  2014-07-05 02:46:00   \n",
       "2  ['A guerra na Síria', 'Desporto', 'Futebol', '...  2014-07-12 16:05:33   \n",
       "3  ['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...  2014-07-27 02:00:00   \n",
       "4  ['Albânia', 'Campeonato da Europa', 'Desporto'...  2014-10-23 00:16:00   \n",
       "\n",
       "                                       url_canonical  \n",
       "0  https://www.publico.pt/2014/06/17/desporto/opi...  \n",
       "1  https://www.publico.pt/2014/07/05/desporto/opi...  \n",
       "2  https://www.publico.pt/2014/07/12/desporto/opi...  \n",
       "3  https://www.publico.pt/2014/07/27/desporto/opi...  \n",
       "4  https://www.publico.pt/2014/10/23/desporto/opi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and normalization\n",
    "\n",
    "The next step is to cleanup our dataset and normalize some data\n",
    "\n",
    "#### Removing non-alphabetic chars\n",
    "\n",
    "Let's start by removing any non-alpha chars, using a regular expression. We'll create a separate corpus (a list of tokens), so that we leave the original dataset untouched.\n",
    "\n",
    "#### Lowercasing\n",
    "\n",
    "We can then apply lowercasing, so that words such as *Amazing*, *AMAZING* and *amazing* all have the same representation.\n",
    "\n",
    "#### Removing stop words\n",
    "\n",
    "Another common step which is sometimes applied is to remove any stop words (words that do not have domain semantics attached). We can use the stop words list provided in NLTK for English:\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Finally, we can apply stemming to further reduce the size of the vocabulary through normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fact não apen frut ignor', 'hav hum jorn investig preocup aprofund contextual histór isenç relat preocup soc urg denunci muit peç real jorn', 'tud cómic fif', 'tod permit organiz faç total absurd sent', 'não faz rir cust poder']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "corpus = []\n",
    "stemmer = RSLPStemmer()\n",
    "for i in range(0, train['tokens'].size):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', train['tokens'][i])\n",
    "    # to lower-case \n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    review = ' '.join([stemmer.stem(w) for w in review.split() if not w in set(stopwords_list)])\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Features and Classes\n",
    "\n",
    "The next step is to obtain the features we will use to train our model.\n",
    "\n",
    "For this, we will use TF-IDF with N-Grams\n",
    "\n",
    "TODO: explore [TfidfVectorizer params](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Number of samples, Number of features): (16743, 69814)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "print(\"(Number of samples, Number of features):\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743,)\n"
     ]
    }
   ],
   "source": [
    "y = train['label']\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Naive Bayes*, the two most effective variants are [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) and [ComplementNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html).\n",
    "- *Logistic Regression*, through scikit-learn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "- *Decision Tree*, through scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class. This model always assigns a probability of 1 to one of the classes.\n",
    "- *Random Forest*, through scikit-learn's [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class.\n",
    "- *Support Vector Machines (SVM)*, through scikit-learn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class. The SVM model also allows you to get probabilities, but for that you need to use the *probability=True* parameter setting in its constructor.\n",
    "- *Perceptron*, through scikit-learn's [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) class. This model does not allow you to get probabilities.\n",
    "- *eXtreme Gradient Boosting*, through [XGBoost](https://xgboost.readthedocs.io/en/stable/).\n",
    "\n",
    "TODO:\n",
    "- Tune parameters\n",
    "- [explore more CVs](https://scikit-learn.org/stable/modules/classes.html?highlight=model_selection#splitter-classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "# Cross Validation and Hyper Tuning\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train machine learning classifiers, we first split the data into training and test sets.\n",
    "We are using 80% of the data to create a train set, and the rest 20% for the test set.\n",
    "We specify the _stratify_ parameter in order to create balanced distribution regarding labels percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13394, 69814) (13394,)\n",
      "(3349, 69814) (3349,)\n",
      "\n",
      "Label distribution in the training set:\n",
      "Value       6481\n",
      "Fact        2930\n",
      "Value(-)    2320\n",
      "Value(+)    1129\n",
      "Policy       534\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution in the test set:\n",
      "Value       1621\n",
      "Fact         733\n",
      "Value(-)     580\n",
      "Value(+)     282\n",
      "Policy       133\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf):\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    stop = time.time()\n",
    "\n",
    "    # Metrics\n",
    "    print(\"Elapsed time: %0.2fs\" % (stop - start))\n",
    "    print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 10.39s\n",
      "\n",
      "Confusion matrix:\n",
      " [[  77    0  652    0    4]\n",
      " [   0    0  133    0    0]\n",
      " [  25    0 1590    0    6]\n",
      " [   5    0  277    0    0]\n",
      " [   7    0  555    0   18]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.68      0.11      0.18       733\n",
      "      Policy       0.00      0.00      0.00       133\n",
      "       Value       0.50      0.98      0.66      1621\n",
      "    Value(+)       0.00      0.00      0.00       282\n",
      "    Value(-)       0.64      0.03      0.06       580\n",
      "\n",
      "    accuracy                           0.50      3349\n",
      "   macro avg       0.36      0.22      0.18      3349\n",
      "weighted avg       0.50      0.50      0.37      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mnb = predict(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 4.13s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 292   26  287   55   73]\n",
      " [   3   63   54   10    3]\n",
      " [ 272   59 1003  101  186]\n",
      " [  36   14  101  110   21]\n",
      " [  64   21  196   14  285]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.40      0.42       733\n",
      "      Policy       0.34      0.47      0.40       133\n",
      "       Value       0.61      0.62      0.61      1621\n",
      "    Value(+)       0.38      0.39      0.38       282\n",
      "    Value(-)       0.50      0.49      0.50       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.45      0.47      0.46      3349\n",
      "weighted avg       0.52      0.52      0.52      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnb = predict(ComplementNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 151.62s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 225    1  432   25   50]\n",
      " [   1   47   77    6    2]\n",
      " [ 178   12 1279   40  112]\n",
      " [  36    4  156   72   14]\n",
      " [  43    1  314    0  222]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.47      0.31      0.37       733\n",
      "      Policy       0.72      0.35      0.47       133\n",
      "       Value       0.57      0.79      0.66      1621\n",
      "    Value(+)       0.50      0.26      0.34       282\n",
      "    Value(-)       0.56      0.38      0.45       580\n",
      "\n",
      "    accuracy                           0.55      3349\n",
      "   macro avg       0.56      0.42      0.46      3349\n",
      "weighted avg       0.54      0.55      0.53      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = predict(SGDClassifier(random_state=0, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3282.09s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 185    0  512    7   29]\n",
      " [   0   11  121    0    1]\n",
      " [ 124    3 1436    5   53]\n",
      " [  23    1  231   25    2]\n",
      " [  33    1  416    0  130]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.51      0.25      0.34       733\n",
      "      Policy       0.69      0.08      0.15       133\n",
      "       Value       0.53      0.89      0.66      1621\n",
      "    Value(+)       0.68      0.09      0.16       282\n",
      "    Value(-)       0.60      0.22      0.33       580\n",
      "\n",
      "    accuracy                           0.53      3349\n",
      "   macro avg       0.60      0.31      0.33      3349\n",
      "weighted avg       0.56      0.53      0.47      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lg = predict(LogisticRegression(random_state=0, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 927.96s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 522    7  161   24   19]\n",
      " [  60   34   33    5    1]\n",
      " [1077   42  419   44   39]\n",
      " [ 193    6   66   12    5]\n",
      " [ 414   13  127    7   19]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.23      0.71      0.35       733\n",
      "      Policy       0.33      0.26      0.29       133\n",
      "       Value       0.52      0.26      0.35      1621\n",
      "    Value(+)       0.13      0.04      0.06       282\n",
      "    Value(-)       0.23      0.03      0.06       580\n",
      "\n",
      "    accuracy                           0.30      3349\n",
      "   macro avg       0.29      0.26      0.22      3349\n",
      "weighted avg       0.37      0.30      0.27      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = predict(SVC(random_state=0, max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 116.62s\n",
      "\n",
      "Confusion matrix:\n",
      " [[  12    1  718    0    2]\n",
      " [   0    5  128    0    0]\n",
      " [   5    1 1608    0    7]\n",
      " [   0    0  282    0    0]\n",
      " [   2    0  577    0    1]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.63      0.02      0.03       733\n",
      "      Policy       0.71      0.04      0.07       133\n",
      "       Value       0.49      0.99      0.65      1621\n",
      "    Value(+)       0.00      0.00      0.00       282\n",
      "    Value(-)       0.10      0.00      0.00       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.39      0.21      0.15      3349\n",
      "weighted avg       0.42      0.49      0.33      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dt = predict(DecisionTreeClassifier(random_state=0, max_depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 22.92s\n",
      "\n",
      "Confusion matrix:\n",
      " [[   0    0  733    0    0]\n",
      " [   0    0  133    0    0]\n",
      " [   0    0 1621    0    0]\n",
      " [   0    0  282    0    0]\n",
      " [   0    0  580    0    0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.00      0.00      0.00       733\n",
      "      Policy       0.00      0.00      0.00       133\n",
      "       Value       0.48      1.00      0.65      1621\n",
      "    Value(+)       0.00      0.00      0.00       282\n",
      "    Value(-)       0.00      0.00      0.00       580\n",
      "\n",
      "    accuracy                           0.48      3349\n",
      "   macro avg       0.10      0.20      0.13      3349\n",
      "weighted avg       0.23      0.48      0.32      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf = predict(RandomForestClassifier(random_state=0, max_depth=5, n_jobs=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 51.21s\n",
      "\n",
      "Confusion matrix:\n",
      " [[  78    0  654    0    1]\n",
      " [   1    1  131    0    0]\n",
      " [  67    0 1550    0    4]\n",
      " [  18    0  259    4    1]\n",
      " [  36    0  538    0    6]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.39      0.11      0.17       733\n",
      "      Policy       1.00      0.01      0.01       133\n",
      "       Value       0.49      0.96      0.65      1621\n",
      "    Value(+)       1.00      0.01      0.03       282\n",
      "    Value(-)       0.50      0.01      0.02       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.68      0.22      0.18      3349\n",
      "weighted avg       0.54      0.49      0.36      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = predict(KNeighborsClassifier(n_neighbors=10, n_jobs=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 128.68s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 192    2  461   28   50]\n",
      " [   2   52   72    5    2]\n",
      " [ 198   18 1241   45  119]\n",
      " [  27    4  174   70    7]\n",
      " [  47    1  330    1  201]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.41      0.26      0.32       733\n",
      "      Policy       0.68      0.39      0.50       133\n",
      "       Value       0.54      0.77      0.64      1621\n",
      "    Value(+)       0.47      0.25      0.32       282\n",
      "    Value(-)       0.53      0.35      0.42       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.53      0.40      0.44      3349\n",
      "weighted avg       0.51      0.52      0.50      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per = predict(Perceptron(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:04] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Elapsed time: 3015.26s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 116    4  576   17   20]\n",
      " [   2   36   91    2    2]\n",
      " [  67   18 1474   17   45]\n",
      " [  14    2  228   34    4]\n",
      " [  22    2  478    3   75]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.52      0.16      0.24       733\n",
      "      Policy       0.58      0.27      0.37       133\n",
      "       Value       0.52      0.91      0.66      1621\n",
      "    Value(+)       0.47      0.12      0.19       282\n",
      "    Value(-)       0.51      0.13      0.21       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.52      0.32      0.33      3349\n",
      "weighted avg       0.52      0.52      0.44      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = predict(xgb.XGBClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO [explore more scoring methods](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(clf, parameter_grid):\n",
    "    cross_validation = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    grid_search = GridSearchCV(clf,\n",
    "                               param_grid=parameter_grid,\n",
    "                               scoring='accuracy',\n",
    "                               cv=cross_validation,\n",
    "                               verbose=4,\n",
    "                               n_jobs=2,\n",
    "                               refit=True)\n",
    "\n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    stop = time.time()\n",
    "    print(f\"Fit time: {stop - start}s\")\n",
    "\n",
    "    print(\"\\nBest score:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, best_model_pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(y_test, best_model_pred))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fit time: 12814.862543821335s\n",
      "\n",
      "Best score: 0.5384496628411\n",
      "Best parameters: {'class_weight': 'balanced', 'loss': 'log', 'penalty': 'l2'}\n",
      "Best estimator: SGDClassifier(class_weight='balanced', loss='log', random_state=0)\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 184    3  444   63   39]\n",
      " [   0   70   48   14    1]\n",
      " [ 113   51 1261  118   78]\n",
      " [  12    8  121  137    4]\n",
      " [  27    6  350   26  171]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.55      0.25      0.34       733\n",
      "      Policy       0.51      0.53      0.52       133\n",
      "       Value       0.57      0.78      0.66      1621\n",
      "    Value(+)       0.38      0.49      0.43       282\n",
      "    Value(-)       0.58      0.29      0.39       580\n",
      "\n",
      "    accuracy                           0.54      3349\n",
      "   macro avg       0.52      0.47      0.47      3349\n",
      "weighted avg       0.55      0.54      0.52      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(random_state=0)\n",
    "\n",
    "# The ‘log’ loss gives logistic regression, ‘perceptron’ is the linear loss used by the perceptron algorithm\n",
    "parameter_grid= {'loss': ['log', 'hinge', 'perceptron'], 'penalty': ['elasticnet', 'l1', 'l2'], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "sgd = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "parameter_grid= {'solver': ['saga'], 'penalty': ['elasticnet', 'l1', 'l2'], 'l1_ratio': [0.5], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "lg = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models per annotator and using ensemble\n",
    "\n",
    "Separating each annotator's judgment will make the task of training the model significantly easier and faster, since we are dealing with less data. At the end of the training, we will use ensemble methods, such as voting, to make a more accurate prediction, by using the predictions of each trained model to reach a consensus on a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_annotated = pd.concat([train['annotator'], pd.DataFrame(X)], axis=1)\n",
    "\n",
    "print(X_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A = X_annotated[X_annotated['annotator'] == 'A']\n",
    "X_A = X_A.drop('annotator', 1)\n",
    "y_A = train.loc[train['annotator'] == 'A', 'label']\n",
    "\n",
    "X_B = X_annotated[X_annotated['annotator'] == 'B']\n",
    "X_B = X_B.drop('annotator', 1)\n",
    "y_B = train.loc[train['annotator'] == 'B', 'label']\n",
    "\n",
    "X_C = X_annotated[X_annotated['annotator'] == 'C']\n",
    "X_C = X_C.drop('annotator', 1)\n",
    "y_C = train.loc[train['annotator'] == 'C', 'label']\n",
    "\n",
    "X_D = X_annotated[X_annotated['annotator'] == 'D']\n",
    "X_D = X_D.drop('annotator', 1)\n",
    "y_D = train.loc[train['annotator'] == 'D', 'label']\n",
    "\n",
    "print(X_A.shape)\n",
    "print(y_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A_tr, X_A_te, y_A_tr, y_A_te = train_test_split(X_A, y_A, test_size = 0.20, random_state=0, stratify=y_A, shuffle=True)\n",
    "\n",
    "X_B_tr, X_B_te, y_B_tr, y_B_te = train_test_split(X_B, y_B, test_size = 0.20, random_state=0, stratify=y_B, shuffle=True)\n",
    "\n",
    "X_C_tr, X_C_te, y_C_tr, y_C_te = train_test_split(X_C, y_C, test_size = 0.20, random_state=0, stratify=y_C, shuffle=True)\n",
    "\n",
    "X_D_tr, X_D_te, y_D_tr, y_D_te = train_test_split(X_D, y_D, test_size = 0.20, random_state=0, stratify=y_D, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can procede to use Voting in order to obtain a better prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "import copy\n",
    "\n",
    "clf_A = SGDClassifier(random_state=0, n_jobs=-1, loss='log')\n",
    "clf_A.fit(X_A_tr, y_A_tr)\n",
    "\n",
    "clf_B = SGDClassifier(random_state=0, n_jobs=-1, loss='log')\n",
    "clf_B.fit(X_B_tr, y_B_tr)\n",
    "\n",
    "clf_C = SGDClassifier(random_state=0, n_jobs=-1, loss='log')\n",
    "clf_C.fit(X_C_tr, y_C_tr)\n",
    "\n",
    "clf_D = SGDClassifier(random_state=0, n_jobs=-1, loss='log')\n",
    "clf_D.fit(X_D_tr, y_D_tr)\n",
    "\n",
    "clf_list = [clf_A, clf_B, clf_C, clf_D]\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=clf_list, fit_base_estimators=False, voting='soft')\n",
    "eclf.fit(X,y)\n",
    "y_pred_vote = eclf.predict(X)\n",
    "print(y_test)\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y, y_pred_vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing minorities\n",
    "\n",
    "To limit the amount of data we will work with, we will group data by tokens and labels, and count the max number of annotators per token-label pair. Next, we will remove the entries of the annotators that will be in a minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = train.groupby(['tokens', 'label']).agg({\n",
    "    'annotator': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "train_no_duplicates = df_tmp.groupby(['tokens'], as_index=False).agg({'annotator': 'max', 'label': 'first'})\n",
    "train_no_duplicates = train_no_duplicates.drop('annotator',1)\n",
    "print(train_no_duplicates)\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, train_no_duplicates['tokens'].size):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', train_no_duplicates['tokens'][i])\n",
    "    # to lower-case \n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    review = ' '.join([stemmer.stem(w) for w in review.split() if not w in set(stopwords_list)])\n",
    "    corpus.append(review)\n",
    "    \n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = train_no_duplicates['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0, stratify=y, shuffle=True)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
