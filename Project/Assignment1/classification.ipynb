{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "Firstly, we must import the dataset into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "train = pd.read_excel('OpArticles_ADUs.xlsx')\n",
    "test = pd.read_excel('OpArticles.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>node</th>\n",
       "      <th>ranges</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2516, 2556]]</td>\n",
       "      <td>O facto não é apenas fruto da ignorância</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[[2568, 2806]]</td>\n",
       "      <td>havia no seu humor mais jornalismo (mais inves...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[[3169, 3190]]</td>\n",
       "      <td>É tudo cómico na FIFA</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[[3198, 3285]]</td>\n",
       "      <td>o que todos nós permitimos que esta organizaçã...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4257, 4296]]</td>\n",
       "      <td>não nos fazem rir à custa dos poderosos</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id annotator  node          ranges  \\\n",
       "0  5d04a31b896a7fea069ef06f         A     0  [[2516, 2556]]   \n",
       "1  5d04a31b896a7fea069ef06f         A     1  [[2568, 2806]]   \n",
       "2  5d04a31b896a7fea069ef06f         A     3  [[3169, 3190]]   \n",
       "3  5d04a31b896a7fea069ef06f         A     4  [[3198, 3285]]   \n",
       "4  5d04a31b896a7fea069ef06f         A     6  [[4257, 4296]]   \n",
       "\n",
       "                                              tokens  label  \n",
       "0           O facto não é apenas fruto da ignorância  Value  \n",
       "1  havia no seu humor mais jornalismo (mais inves...  Value  \n",
       "2                              É tudo cómico na FIFA  Value  \n",
       "3  o que todos nós permitimos que esta organizaçã...  Value  \n",
       "4            não nos fazem rir à custa dos poderosos  Value  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>body</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>topics</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>url_canonical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>Pouco pão e muito circo, morte e bocejo</td>\n",
       "      <td>['José Vítor Malheiros']</td>\n",
       "      <td>O poeta espanhol António Machado escrevia, uns...</td>\n",
       "      <td>É tudo cómico na FIFA, porque todos os dias a ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Desporto', ...</td>\n",
       "      <td>2014-06-17 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/06/17/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a3fc896a7fea069f0717</td>\n",
       "      <td>Portugal nos Mundiais de Futebol de 2010 e 2014</td>\n",
       "      <td>['Rui J. Baptista']</td>\n",
       "      <td>“O mais excelente quadro posto a uma luz logo ...</td>\n",
       "      <td>Deve ser evidenciado o clima favorável criado ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Coreia do N...</td>\n",
       "      <td>2014-07-05 02:46:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/05/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a455896a7fea069f07ab</td>\n",
       "      <td>Futebol, guerra, religião</td>\n",
       "      <td>['Fernando Belo']</td>\n",
       "      <td>1. As sociedades humanas parecem ser regidas p...</td>\n",
       "      <td>O futebol parece ser um sucedâneo quer da lei ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['A guerra na Síria', 'Desporto', 'Futebol', '...</td>\n",
       "      <td>2014-07-12 16:05:33</td>\n",
       "      <td>https://www.publico.pt/2014/07/12/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a52f896a7fea069f0921</td>\n",
       "      <td>As razões do Qatar para acolher o Mundial em 2022</td>\n",
       "      <td>['Hamad bin Khalifa bin Ahmad Al Thani']</td>\n",
       "      <td>Este foi um Mundial incrível. Vimos actuações ...</td>\n",
       "      <td>Queremos cooperar plenamente com a investigaçã...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...</td>\n",
       "      <td>2014-07-27 02:00:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/27/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a8d7896a7fea069f6997</td>\n",
       "      <td>A política no campo de futebol</td>\n",
       "      <td>['Carlos Nolasco']</td>\n",
       "      <td>O futebol sempre foi um jogo aparentemente sim...</td>\n",
       "      <td>Retirar a expressão política do futebol é reti...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Albânia', 'Campeonato da Europa', 'Desporto'...</td>\n",
       "      <td>2014-10-23 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/10/23/desporto/opi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  \\\n",
       "0  5d04a31b896a7fea069ef06f   \n",
       "1  5d04a3fc896a7fea069f0717   \n",
       "2  5d04a455896a7fea069f07ab   \n",
       "3  5d04a52f896a7fea069f0921   \n",
       "4  5d04a8d7896a7fea069f6997   \n",
       "\n",
       "                                               title  \\\n",
       "0            Pouco pão e muito circo, morte e bocejo   \n",
       "1    Portugal nos Mundiais de Futebol de 2010 e 2014   \n",
       "2                          Futebol, guerra, religião   \n",
       "3  As razões do Qatar para acolher o Mundial em 2022   \n",
       "4                     A política no campo de futebol   \n",
       "\n",
       "                                    authors  \\\n",
       "0                  ['José Vítor Malheiros']   \n",
       "1                       ['Rui J. Baptista']   \n",
       "2                         ['Fernando Belo']   \n",
       "3  ['Hamad bin Khalifa bin Ahmad Al Thani']   \n",
       "4                        ['Carlos Nolasco']   \n",
       "\n",
       "                                                body  \\\n",
       "0  O poeta espanhol António Machado escrevia, uns...   \n",
       "1  “O mais excelente quadro posto a uma luz logo ...   \n",
       "2  1. As sociedades humanas parecem ser regidas p...   \n",
       "3  Este foi um Mundial incrível. Vimos actuações ...   \n",
       "4  O futebol sempre foi um jogo aparentemente sim...   \n",
       "\n",
       "                                    meta_description  topics  \\\n",
       "0  É tudo cómico na FIFA, porque todos os dias a ...  Sports   \n",
       "1  Deve ser evidenciado o clima favorável criado ...  Sports   \n",
       "2  O futebol parece ser um sucedâneo quer da lei ...  Sports   \n",
       "3  Queremos cooperar plenamente com a investigaçã...  Sports   \n",
       "4  Retirar a expressão política do futebol é reti...  Sports   \n",
       "\n",
       "                                            keywords         publish_date  \\\n",
       "0  ['Brasil', 'Campeonato do Mundo', 'Desporto', ...  2014-06-17 00:16:00   \n",
       "1  ['Brasil', 'Campeonato do Mundo', 'Coreia do N...  2014-07-05 02:46:00   \n",
       "2  ['A guerra na Síria', 'Desporto', 'Futebol', '...  2014-07-12 16:05:33   \n",
       "3  ['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...  2014-07-27 02:00:00   \n",
       "4  ['Albânia', 'Campeonato da Europa', 'Desporto'...  2014-10-23 00:16:00   \n",
       "\n",
       "                                       url_canonical  \n",
       "0  https://www.publico.pt/2014/06/17/desporto/opi...  \n",
       "1  https://www.publico.pt/2014/07/05/desporto/opi...  \n",
       "2  https://www.publico.pt/2014/07/12/desporto/opi...  \n",
       "3  https://www.publico.pt/2014/07/27/desporto/opi...  \n",
       "4  https://www.publico.pt/2014/10/23/desporto/opi...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and normalization\n",
    "\n",
    "The next step is to cleanup our dataset and normalize some data\n",
    "\n",
    "#### Removing non-alphabetic chars\n",
    "\n",
    "Let's start by removing any non-alpha chars, using a regular expression. We'll create a separate corpus (a list of tokens), so that we leave the original dataset untouched.\n",
    "\n",
    "#### Lowercasing\n",
    "\n",
    "We can then apply lowercasing, so that words such as *Amazing*, *AMAZING* and *amazing* all have the same representation.\n",
    "\n",
    "#### Removing stop words\n",
    "\n",
    "Another common step which is sometimes applied is to remove any stop words (words that do not have domain semantics attached). We can use the stop words list provided in NLTK for English:\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Finally, we can apply stemming to further reduce the size of the vocabulary through normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fact não apen frut ignor', 'hav hum jorn investig preocup aprofund contextual histór isenç relat preocup soc urg denunci muit peç real jorn', 'tud cómic fif', 'tod permit organiz faç total absurd sent', 'não faz rir cust poder']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "corpus = []\n",
    "stemmer = RSLPStemmer()\n",
    "for i in range(0, train['tokens'].size):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', train['tokens'][i])\n",
    "    # to lower-case \n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    review = ' '.join([stemmer.stem(w) for w in review.split() if not w in set(stopwords_list)])\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Features and Classes\n",
    "\n",
    "The next step is to obtain the features we will use to train our model.\n",
    "\n",
    "For this, we will use TF-IDF with N-Grams\n",
    "\n",
    "TODO: explore [TfidfVectorizer params](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Number of samples, Number of features): (16743, 19262)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.8)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "print(\"(Number of samples, Number of features):\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743,)\n"
     ]
    }
   ],
   "source": [
    "y = train['label']\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Naive Bayes*, the two most effective variants are [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) and [ComplementNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html).\n",
    "- *Logistic Regression*, through scikit-learn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "- *Decision Tree*, through scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class. This model always assigns a probability of 1 to one of the classes.\n",
    "- *Random Forest*, through scikit-learn's [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class.\n",
    "- *Support Vector Machines (SVM)*, through scikit-learn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class. The SVM model also allows you to get probabilities, but for that you need to use the *probability=True* parameter setting in its constructor.\n",
    "- *Perceptron*, through scikit-learn's [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) class. This model does not allow you to get probabilities.\n",
    "- *eXtreme Gradient Boosting*, through [XGBoost](https://xgboost.readthedocs.io/en/stable/).\n",
    "\n",
    "TODO:\n",
    "- Tune parameters\n",
    "- [explore more CVs](https://scikit-learn.org/stable/modules/classes.html?highlight=model_selection#splitter-classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "# Cross Validation and Hyper Tuning\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train machine learning classifiers, we first split the data into training and test sets.\n",
    "We are using 80% of the data to create a train set, and the rest 20% for the test set.\n",
    "We specify the _stratify_ parameter in order to create balanced distribution regarding labels percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13394, 19262) (13394,)\n",
      "(3349, 19262) (3349,)\n",
      "\n",
      "Label distribution in the training set:\n",
      "Value       6481\n",
      "Fact        2930\n",
      "Value(-)    2320\n",
      "Value(+)    1129\n",
      "Policy       534\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution in the test set:\n",
      "Value       1621\n",
      "Fact         733\n",
      "Value(-)     580\n",
      "Value(+)     282\n",
      "Policy       133\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf):\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    stop = time.time()\n",
    "\n",
    "    # Metrics\n",
    "    print(\"Elapsed time: %0.2fs\" % (stop - start))\n",
    "    print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.02s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 116    0  598    1   18]\n",
      " [   0    0  133    0    0]\n",
      " [  74    0 1517    0   30]\n",
      " [  15    0  260    5    2]\n",
      " [  14    0  496    0   70]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.53      0.16      0.24       733\n",
      "      Policy       0.00      0.00      0.00       133\n",
      "       Value       0.50      0.94      0.66      1621\n",
      "    Value(+)       0.83      0.02      0.03       282\n",
      "    Value(-)       0.58      0.12      0.20       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.49      0.25      0.23      3349\n",
      "weighted avg       0.53      0.51      0.41      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mnb = predict(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.89s\n",
      "\n",
      "Confusion matrix:\n",
      " [[301  16 307  40  69]\n",
      " [  4  58  58   8   5]\n",
      " [286  54 977  95 209]\n",
      " [ 39  13  96 115  19]\n",
      " [ 75  10 201  13 281]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.43      0.41      0.42       733\n",
      "      Policy       0.38      0.44      0.41       133\n",
      "       Value       0.60      0.60      0.60      1621\n",
      "    Value(+)       0.42      0.41      0.42       282\n",
      "    Value(-)       0.48      0.48      0.48       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.46      0.47      0.47      3349\n",
      "weighted avg       0.52      0.52      0.52      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnb = predict(ComplementNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 27.14s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 230    3  418   28   54]\n",
      " [   7   38   72   14    2]\n",
      " [ 182   18 1249   48  124]\n",
      " [  32    8  143   84   15]\n",
      " [  47    2  310    6  215]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.46      0.31      0.37       733\n",
      "      Policy       0.55      0.29      0.38       133\n",
      "       Value       0.57      0.77      0.66      1621\n",
      "    Value(+)       0.47      0.30      0.36       282\n",
      "    Value(-)       0.52      0.37      0.43       580\n",
      "\n",
      "    accuracy                           0.54      3349\n",
      "   macro avg       0.51      0.41      0.44      3349\n",
      "weighted avg       0.53      0.54      0.52      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = predict(SGDClassifier(random_state=0, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 198.74s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 217    1  472    7   36]\n",
      " [   4   14  110    4    1]\n",
      " [ 158    4 1372    5   82]\n",
      " [  31    2  205   34   10]\n",
      " [  47    1  389    0  143]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.47      0.30      0.36       733\n",
      "      Policy       0.64      0.11      0.18       133\n",
      "       Value       0.54      0.85      0.66      1621\n",
      "    Value(+)       0.68      0.12      0.20       282\n",
      "    Value(-)       0.53      0.25      0.34       580\n",
      "\n",
      "    accuracy                           0.53      3349\n",
      "   macro avg       0.57      0.32      0.35      3349\n",
      "weighted avg       0.54      0.53      0.48      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lg = predict(LogisticRegression(random_state=0, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 225.82s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 189   19  518    6    1]\n",
      " [  13   41   78    1    0]\n",
      " [ 402   66 1141   10    2]\n",
      " [  82   12  184    4    0]\n",
      " [ 134   14  424    6    2]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.23      0.26      0.24       733\n",
      "      Policy       0.27      0.31      0.29       133\n",
      "       Value       0.49      0.70      0.58      1621\n",
      "    Value(+)       0.15      0.01      0.03       282\n",
      "    Value(-)       0.40      0.00      0.01       580\n",
      "\n",
      "    accuracy                           0.41      3349\n",
      "   macro avg       0.31      0.26      0.23      3349\n",
      "weighted avg       0.38      0.41      0.35      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = predict(SVC(random_state=0, max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 21.25s\n",
      "\n",
      "Confusion matrix:\n",
      " [[  22    3  704    1    3]\n",
      " [   1   17  113    1    1]\n",
      " [  13    6 1597    0    5]\n",
      " [   0    0  280    1    1]\n",
      " [   9    0  568    0    3]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.49      0.03      0.06       733\n",
      "      Policy       0.65      0.13      0.21       133\n",
      "       Value       0.49      0.99      0.65      1621\n",
      "    Value(+)       0.33      0.00      0.01       282\n",
      "    Value(-)       0.23      0.01      0.01       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.44      0.23      0.19      3349\n",
      "weighted avg       0.44      0.49      0.34      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = predict(DecisionTreeClassifier(random_state=0, max_depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.00s\n",
      "\n",
      "Confusion matrix:\n",
      " [[   0    0  733    0    0]\n",
      " [   0    0  133    0    0]\n",
      " [   0    0 1621    0    0]\n",
      " [   0    0  282    0    0]\n",
      " [   0    0  580    0    0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.00      0.00      0.00       733\n",
      "      Policy       0.00      0.00      0.00       133\n",
      "       Value       0.48      1.00      0.65      1621\n",
      "    Value(+)       0.00      0.00      0.00       282\n",
      "    Value(-)       0.00      0.00      0.00       580\n",
      "\n",
      "    accuracy                           0.48      3349\n",
      "   macro avg       0.10      0.20      0.13      3349\n",
      "weighted avg       0.23      0.48      0.32      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf = predict(RandomForestClassifier(random_state=0, max_depth=5, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 14.77s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 199    0  451   33   50]\n",
      " [  36    1   76    8   12]\n",
      " [ 344    0 1120   66   91]\n",
      " [  60    0  169   36   17]\n",
      " [ 110    0  352   26   92]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.27      0.27      0.27       733\n",
      "      Policy       1.00      0.01      0.01       133\n",
      "       Value       0.52      0.69      0.59      1621\n",
      "    Value(+)       0.21      0.13      0.16       282\n",
      "    Value(-)       0.35      0.16      0.22       580\n",
      "\n",
      "    accuracy                           0.43      3349\n",
      "   macro avg       0.47      0.25      0.25      3349\n",
      "weighted avg       0.43      0.43      0.40      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = predict(KNeighborsClassifier(n_neighbors=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 48.11s\n",
      "\n",
      "Confusion matrix:\n",
      " [[284  12 307  37  93]\n",
      " [  4  67  45   9   8]\n",
      " [333  40 931  66 251]\n",
      " [ 46  13 114  86  23]\n",
      " [ 89   4 197   4 286]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.38      0.39      0.38       733\n",
      "      Policy       0.49      0.50      0.50       133\n",
      "       Value       0.58      0.57      0.58      1621\n",
      "    Value(+)       0.43      0.30      0.36       282\n",
      "    Value(-)       0.43      0.49      0.46       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.46      0.45      0.46      3349\n",
      "weighted avg       0.50      0.49      0.49      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per = predict(Perceptron(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:43:15] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Elapsed time: 589.44s\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 119    2  576   15   21]\n",
      " [   2   35   89    4    3]\n",
      " [  71   18 1463   14   55]\n",
      " [  13    2  238   25    4]\n",
      " [  21    3  465    2   89]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.53      0.16      0.25       733\n",
      "      Policy       0.58      0.26      0.36       133\n",
      "       Value       0.52      0.90      0.66      1621\n",
      "    Value(+)       0.42      0.09      0.15       282\n",
      "    Value(-)       0.52      0.15      0.24       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.51      0.31      0.33      3349\n",
      "weighted avg       0.51      0.52      0.44      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = predict(xgb.XGBClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "To further better the training of the model we explored different parameters within each classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO [explore more scoring methods](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(clf, parameter_grid):\n",
    "    cross_validation = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    grid_search = GridSearchCV(clf,\n",
    "                               param_grid=parameter_grid,\n",
    "                               scoring='accuracy',\n",
    "                               cv=cross_validation,\n",
    "                               verbose=4,\n",
    "                               n_jobs=2,\n",
    "                               refit=True)\n",
    "\n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    stop = time.time()\n",
    "    print(f\"Fit time: {stop - start}s\")\n",
    "\n",
    "    print(\"\\nBest score:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, best_model_pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(y_test, best_model_pred))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fit time: 2206.660979270935s\n",
      "\n",
      "Best score: 0.5224735802291549\n",
      "Best parameters: {'class_weight': None, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "Best estimator: SGDClassifier(early_stopping=True, n_jobs=-1, random_state=0)\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 146    2  515   11   59]\n",
      " [   1   33   90    5    4]\n",
      " [  95   11 1373   13  129]\n",
      " [  18    2  192   41   29]\n",
      " [  20    1  363    0  196]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.52      0.20      0.29       733\n",
      "      Policy       0.67      0.25      0.36       133\n",
      "       Value       0.54      0.85      0.66      1621\n",
      "    Value(+)       0.59      0.15      0.23       282\n",
      "    Value(-)       0.47      0.34      0.39       580\n",
      "\n",
      "    accuracy                           0.53      3349\n",
      "   macro avg       0.56      0.36      0.39      3349\n",
      "weighted avg       0.53      0.53      0.49      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(random_state=0, n_jobs=-1, early_stopping=True)\n",
    "\n",
    "# The ‘log’ loss gives logistic regression, ‘perceptron’ is the linear loss used by the perceptron algorithm\n",
    "parameter_grid= {'loss': ['log', 'hinge', 'perceptron'], 'penalty': ['elasticnet', 'l1', 'l2'], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "sgd = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "parameter_grid= {'solver': ['saga'], 'penalty': ['elasticnet', 'l1', 'l2'], 'l1_ratio': [0.5], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "lg = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models per annotator and using ensemble\n",
    "\n",
    "Separating each annotator's judgment will make the task of training the model significantly easier and faster, since we are dealing with less data. At the end of the training, we will use ensemble methods, such as voting, to make a more accurate prediction, by using the predictions of each trained model to reach a consensus on a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      annotator    0    1    2    3    4    5    6    7    8  ...  19252  \\\n",
      "0             A  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "1             A  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "2             A  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "3             A  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4             A  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...   \n",
      "16738         D  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "16739         D  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "16740         D  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "16741         D  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "16742         D  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "\n",
      "       19253  19254  19255  19256  19257  19258  19259  19260  19261  \n",
      "0        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "1        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "2        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "3        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "4        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "16738    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "16739    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "16740    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "16741    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "16742    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "\n",
      "[16743 rows x 19263 columns]\n"
     ]
    }
   ],
   "source": [
    "X_annotated = pd.concat([train['annotator'], pd.DataFrame(X)], axis=1)\n",
    "\n",
    "print(X_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3335, 19262)\n",
      "(3335,)\n"
     ]
    }
   ],
   "source": [
    "X_A = X_annotated[X_annotated['annotator'] == 'A']\n",
    "X_A = X_A.drop('annotator', 1)\n",
    "y_A = train.loc[train['annotator'] == 'A', 'label']\n",
    "\n",
    "X_B = X_annotated[X_annotated['annotator'] == 'B']\n",
    "X_B = X_B.drop('annotator', 1)\n",
    "y_B = train.loc[train['annotator'] == 'B', 'label']\n",
    "\n",
    "X_C = X_annotated[X_annotated['annotator'] == 'C']\n",
    "X_C = X_C.drop('annotator', 1)\n",
    "y_C = train.loc[train['annotator'] == 'C', 'label']\n",
    "\n",
    "X_D = X_annotated[X_annotated['annotator'] == 'D']\n",
    "X_D = X_D.drop('annotator', 1)\n",
    "y_D = train.loc[train['annotator'] == 'D', 'label']\n",
    "\n",
    "print(X_A.shape)\n",
    "print(y_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A_tr, X_A_te, y_A_tr, y_A_te = train_test_split(X_A, y_A, test_size=0.20, random_state=0, stratify=y_A)\n",
    "\n",
    "X_B_tr, X_B_te, y_B_tr, y_B_te = train_test_split(X_B, y_B, test_size=0.20, random_state=0, stratify=y_B)\n",
    "\n",
    "X_C_tr, X_C_te, y_C_tr, y_C_te = train_test_split(X_C, y_C, test_size=0.20, random_state=0, stratify=y_C)\n",
    "\n",
    "X_D_tr, X_D_te, y_D_tr, y_D_te = train_test_split(X_D, y_D, test_size=0.20, random_state=0, stratify=y_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can procede to use Voting in order to obtain a better prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlxtend\\classifier\\ensemble_vote.py:166: UserWarning: fit_base_estimators=False enforces use_clones to be `False`\n",
      "  warnings.warn(\"fit_base_estimators=False \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2674     Value(-)\n",
      "7609     Value(-)\n",
      "10747    Value(-)\n",
      "12931        Fact\n",
      "4107     Value(+)\n",
      "           ...   \n",
      "8773        Value\n",
      "13538    Value(-)\n",
      "1782        Value\n",
      "9772        Value\n",
      "11212       Value\n",
      "Name: label, Length: 3349, dtype: object\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.80      0.21      0.34      3663\n",
      "      Policy       0.87      0.04      0.08       667\n",
      "       Value       0.52      0.99      0.68      8102\n",
      "    Value(+)       0.93      0.04      0.07      1411\n",
      "    Value(-)       0.89      0.13      0.22      2900\n",
      "\n",
      "    accuracy                           0.55     16743\n",
      "   macro avg       0.80      0.28      0.28     16743\n",
      "weighted avg       0.69      0.55      0.45     16743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "import copy\n",
    "\n",
    "clf_A = SGDClassifier(random_state=0, n_jobs=-1, loss=\"log\")\n",
    "clf_A.fit(X_A_tr, y_A_tr)\n",
    "\n",
    "clf_B = SGDClassifier(random_state=0, n_jobs=-1, loss=\"log\")\n",
    "clf_B.fit(X_B_tr, y_B_tr)\n",
    "\n",
    "clf_C = SGDClassifier(random_state=0, n_jobs=-1, loss=\"log\")\n",
    "clf_C.fit(X_C_tr, y_C_tr)\n",
    "\n",
    "clf_D = SGDClassifier(random_state=0, n_jobs=-1, loss=\"log\")\n",
    "clf_D.fit(X_D_tr, y_D_tr)\n",
    "\n",
    "clf_list = [clf_A, clf_B, clf_C, clf_D]\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=clf_list, fit_base_estimators=False, voting='soft')\n",
    "eclf.fit(X,y)\n",
    "y_pred_vote = eclf.predict(X)\n",
    "print(y_test)\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y, y_pred_vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing minorities\n",
    "\n",
    "Next, we will remove the entries of the annotators that will be in a minority, refering to the classification of a label. Our idea was that if a majority of the annotators agrees \n",
    "this will limit the amount of data we will work with, by grouping data by tokens and labels, and count the max number of annotators per token-label pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tokens  label\n",
      "0      \"porque é mais prático e, o que é pior, para t...   Fact\n",
      "1      \"revolução\" é um termo frequente na literatura...   Fact\n",
      "2      (In)felizmente, essas relações acabam, mais ce...   Fact\n",
      "3                    (em 2017 foram quatro vezes maiores   Fact\n",
      "4      (pelo que julgo poder deduzir) não está obriga...   Fact\n",
      "...                                                  ...    ...\n",
      "12003  “tratando-se de pares cujos elementos pertence...   Fact\n",
      "12004  “um instituto universitário especializado com ...   Fact\n",
      "12005  “um militar na polícia, nem consegue ser bom p...  Value\n",
      "12006                           “Água pura, cristalina.”   Fact\n",
      "12007      “área verde de enquadramento de espaço canal”  Value\n",
      "\n",
      "[12008 rows x 2 columns]\n",
      "(9606, 8054) (9606,)\n",
      "(2402, 8054) (2402,)\n",
      "\n",
      "Label distribution in the training set:\n",
      "Value       4537\n",
      "Fact        2549\n",
      "Value(-)    1412\n",
      "Value(+)     678\n",
      "Policy       430\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution in the test set:\n",
      "Value       1135\n",
      "Fact         637\n",
      "Value(-)     353\n",
      "Value(+)     170\n",
      "Policy       107\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_tmp = train.groupby(['tokens', 'label']).agg({\n",
    "    'annotator': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "train_no_duplicates = df_tmp.groupby(['tokens'], as_index=False).agg({'annotator': 'max', 'label': 'first'})\n",
    "train_no_duplicates = train_no_duplicates.drop('annotator',1)\n",
    "print(train_no_duplicates)\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, train_no_duplicates['tokens'].size):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', train_no_duplicates['tokens'][i])\n",
    "    # to lower-case \n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    review = ' '.join([stemmer.stem(w) for w in review.split() if not w in set(stopwords_list)])\n",
    "    corpus.append(review)\n",
    "    \n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = train_no_duplicates['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y, shuffle=True)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
