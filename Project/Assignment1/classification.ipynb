{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "Firstly, we must import the dataset into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "train = pd.read_excel('OpArticles_ADUs.xlsx')\n",
    "test = pd.read_excel('OpArticles.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>node</th>\n",
       "      <th>ranges</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2516, 2556]]</td>\n",
       "      <td>O facto não é apenas fruto da ignorância</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[[2568, 2806]]</td>\n",
       "      <td>havia no seu humor mais jornalismo (mais inves...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[[3169, 3190]]</td>\n",
       "      <td>É tudo cómico na FIFA</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[[3198, 3285]]</td>\n",
       "      <td>o que todos nós permitimos que esta organizaçã...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4257, 4296]]</td>\n",
       "      <td>não nos fazem rir à custa dos poderosos</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id annotator  node          ranges  \\\n",
       "0  5d04a31b896a7fea069ef06f         A     0  [[2516, 2556]]   \n",
       "1  5d04a31b896a7fea069ef06f         A     1  [[2568, 2806]]   \n",
       "2  5d04a31b896a7fea069ef06f         A     3  [[3169, 3190]]   \n",
       "3  5d04a31b896a7fea069ef06f         A     4  [[3198, 3285]]   \n",
       "4  5d04a31b896a7fea069ef06f         A     6  [[4257, 4296]]   \n",
       "\n",
       "                                              tokens  label  \n",
       "0           O facto não é apenas fruto da ignorância  Value  \n",
       "1  havia no seu humor mais jornalismo (mais inves...  Value  \n",
       "2                              É tudo cómico na FIFA  Value  \n",
       "3  o que todos nós permitimos que esta organizaçã...  Value  \n",
       "4            não nos fazem rir à custa dos poderosos  Value  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>body</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>topics</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>url_canonical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>Pouco pão e muito circo, morte e bocejo</td>\n",
       "      <td>['José Vítor Malheiros']</td>\n",
       "      <td>O poeta espanhol António Machado escrevia, uns...</td>\n",
       "      <td>É tudo cómico na FIFA, porque todos os dias a ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Desporto', ...</td>\n",
       "      <td>2014-06-17 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/06/17/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a3fc896a7fea069f0717</td>\n",
       "      <td>Portugal nos Mundiais de Futebol de 2010 e 2014</td>\n",
       "      <td>['Rui J. Baptista']</td>\n",
       "      <td>“O mais excelente quadro posto a uma luz logo ...</td>\n",
       "      <td>Deve ser evidenciado o clima favorável criado ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Coreia do N...</td>\n",
       "      <td>2014-07-05 02:46:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/05/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a455896a7fea069f07ab</td>\n",
       "      <td>Futebol, guerra, religião</td>\n",
       "      <td>['Fernando Belo']</td>\n",
       "      <td>1. As sociedades humanas parecem ser regidas p...</td>\n",
       "      <td>O futebol parece ser um sucedâneo quer da lei ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['A guerra na Síria', 'Desporto', 'Futebol', '...</td>\n",
       "      <td>2014-07-12 16:05:33</td>\n",
       "      <td>https://www.publico.pt/2014/07/12/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a52f896a7fea069f0921</td>\n",
       "      <td>As razões do Qatar para acolher o Mundial em 2022</td>\n",
       "      <td>['Hamad bin Khalifa bin Ahmad Al Thani']</td>\n",
       "      <td>Este foi um Mundial incrível. Vimos actuações ...</td>\n",
       "      <td>Queremos cooperar plenamente com a investigaçã...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...</td>\n",
       "      <td>2014-07-27 02:00:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/27/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a8d7896a7fea069f6997</td>\n",
       "      <td>A política no campo de futebol</td>\n",
       "      <td>['Carlos Nolasco']</td>\n",
       "      <td>O futebol sempre foi um jogo aparentemente sim...</td>\n",
       "      <td>Retirar a expressão política do futebol é reti...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Albânia', 'Campeonato da Europa', 'Desporto'...</td>\n",
       "      <td>2014-10-23 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/10/23/desporto/opi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  \\\n",
       "0  5d04a31b896a7fea069ef06f   \n",
       "1  5d04a3fc896a7fea069f0717   \n",
       "2  5d04a455896a7fea069f07ab   \n",
       "3  5d04a52f896a7fea069f0921   \n",
       "4  5d04a8d7896a7fea069f6997   \n",
       "\n",
       "                                               title  \\\n",
       "0            Pouco pão e muito circo, morte e bocejo   \n",
       "1    Portugal nos Mundiais de Futebol de 2010 e 2014   \n",
       "2                          Futebol, guerra, religião   \n",
       "3  As razões do Qatar para acolher o Mundial em 2022   \n",
       "4                     A política no campo de futebol   \n",
       "\n",
       "                                    authors  \\\n",
       "0                  ['José Vítor Malheiros']   \n",
       "1                       ['Rui J. Baptista']   \n",
       "2                         ['Fernando Belo']   \n",
       "3  ['Hamad bin Khalifa bin Ahmad Al Thani']   \n",
       "4                        ['Carlos Nolasco']   \n",
       "\n",
       "                                                body  \\\n",
       "0  O poeta espanhol António Machado escrevia, uns...   \n",
       "1  “O mais excelente quadro posto a uma luz logo ...   \n",
       "2  1. As sociedades humanas parecem ser regidas p...   \n",
       "3  Este foi um Mundial incrível. Vimos actuações ...   \n",
       "4  O futebol sempre foi um jogo aparentemente sim...   \n",
       "\n",
       "                                    meta_description  topics  \\\n",
       "0  É tudo cómico na FIFA, porque todos os dias a ...  Sports   \n",
       "1  Deve ser evidenciado o clima favorável criado ...  Sports   \n",
       "2  O futebol parece ser um sucedâneo quer da lei ...  Sports   \n",
       "3  Queremos cooperar plenamente com a investigaçã...  Sports   \n",
       "4  Retirar a expressão política do futebol é reti...  Sports   \n",
       "\n",
       "                                            keywords         publish_date  \\\n",
       "0  ['Brasil', 'Campeonato do Mundo', 'Desporto', ...  2014-06-17 00:16:00   \n",
       "1  ['Brasil', 'Campeonato do Mundo', 'Coreia do N...  2014-07-05 02:46:00   \n",
       "2  ['A guerra na Síria', 'Desporto', 'Futebol', '...  2014-07-12 16:05:33   \n",
       "3  ['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...  2014-07-27 02:00:00   \n",
       "4  ['Albânia', 'Campeonato da Europa', 'Desporto'...  2014-10-23 00:16:00   \n",
       "\n",
       "                                       url_canonical  \n",
       "0  https://www.publico.pt/2014/06/17/desporto/opi...  \n",
       "1  https://www.publico.pt/2014/07/05/desporto/opi...  \n",
       "2  https://www.publico.pt/2014/07/12/desporto/opi...  \n",
       "3  https://www.publico.pt/2014/07/27/desporto/opi...  \n",
       "4  https://www.publico.pt/2014/10/23/desporto/opi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stopwords and Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Esta unidade curricular de Processamento de Linguagem Natural é extremamente divertida e útil porque nos ajuda a desenvolver novas competências no campo da Aprendizagem Computacional não muito bem lecionada pelo Carlos Sexy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portuguese Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "\n",
    "# Remover algumas palavras da lista, p.ex. \"não\"\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Esta unidade curricular Processamento Linguagem Natural extremamente divertida útil porque ajuda desenvolver novas competências campo Aprendizagem Computacional não bem lecionada Carlos Sexy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in text.split() if w not in stopwords_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying some Portuguese Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball Portugue Stemmer:\n",
      " esta unidad curricul de process de linguag natural é extrem divert e útil porqu nos ajud a desenvolv nov competent no camp da aprendizag computacional nã muit bem lecion pel carl sexy\n",
      "\n",
      "RSLP Stemmer:\n",
      " est unidad curricul de process de lingu natur é extrem divert e útil porqu no ajud a desenvolv nov compet no camp da aprendiz computac não muit bem lecion pel carl sexy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "snb = PortugueseStemmer()\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "print(\"Snowball Portugue Stemmer:\\n\", ' '.join([snb.stem(w) for w in text.split()]))\n",
    "print(\"\\nRSLP Stemmer:\\n\",' '.join([rslp.stem(w) for w in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and normalization\n",
    "\n",
    "The next step is to cleanup our dataset and normalize some data\n",
    "\n",
    "#### Removing non-alphabetic chars\n",
    "\n",
    "Let's start by removing any non-alpha chars, using a regular expression. We'll create a separate corpus (a list of tokens), so that we leave the original dataset untouched.\n",
    "\n",
    "#### Lowercasing\n",
    "\n",
    "We can then apply lowercasing, so that words such as *Amazing*, *AMAZING* and *amazing* all have the same representation.\n",
    "\n",
    "#### Removing stop words\n",
    "\n",
    "Another common step which is sometimes applied is to remove any stop words (words that do not have domain semantics attached). We can use the stop words list provided in NLTK for English:\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Finally, we can apply stemming to further reduce the size of the vocabulary through normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fact n apen frut ign nci', 'hav hum jorn investig preocup aprofund contextual hist ria isen relat preocup soc urg nci denunci muit pe real jorn stic', 'tud c mic fif', 'tod n s permit organiz fa total absurd sent', 'n faz rir cust poder']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = []\n",
    "stemmer = rslp\n",
    "for i in range(0, train['tokens'].size):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z]', ' ', train['tokens'][i])\n",
    "    # to lower-case \n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    review = ' '.join([stemmer.stem(w) for w in review.split() if not w in set(stopwords_list)])\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Features and Classes\n",
    "\n",
    "The next step is to obtain the features we will use to train our model.\n",
    "\n",
    "For this, we will use TF-IDF with N-Grams, but firstly, we will divide our tokens from our label:\n",
    "\n",
    "TODO: explore [TfidfVectorizer params](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Number of samples, Number of features): (16743, 7601)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "print(\"(Number of samples, Number of features):\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743,)\n"
     ]
    }
   ],
   "source": [
    "y = train['label']\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Naive Bayes*, the two most effective variants are [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) and [ComplementNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html).\n",
    "- *Logistic Regression*, through scikit-learn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class.\n",
    "- *Decision Tree*, through scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class. This model always assigns a probability of 1 to one of the classes.\n",
    "- *Random Forest*, through scikit-learn's [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class.\n",
    "- *Support Vector Machines (SVM)*, through scikit-learn's [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class. The SVM model also allows you to get probabilities, but for that you need to use the *probability=True* parameter setting in its constructor.\n",
    "- *Perceptron*, through scikit-learn's [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) class. This model does not allow you to get probabilities.\n",
    "- *eXtreme Gradient Boosting*, through [XGBoost](https://xgboost.readthedocs.io/en/stable/).\n",
    "\n",
    "TODO:\n",
    "- Tune parameters\n",
    "- [explore more CVs](https://scikit-learn.org/stable/modules/classes.html?highlight=model_selection#splitter-classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Cross Validation and Hyper Tuning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train machine learning classifiers, we first split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13394, 7601) (13394,)\n",
      "(3349, 7601) (3349,)\n",
      "\n",
      "Label distribution in the training set:\n",
      "Value       6481\n",
      "Fact        2930\n",
      "Value(-)    2320\n",
      "Value(+)    1129\n",
      "Policy       534\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution in the test set:\n",
      "Value       1621\n",
      "Fact         733\n",
      "Value(-)     580\n",
      "Value(+)     282\n",
      "Policy       133\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0, stratify=y, shuffle=True)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO [explore more scoring methods](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(clf, parameter_grid):\n",
    "    cross_validation = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    grid_search = GridSearchCV(clf,\n",
    "                               param_grid=parameter_grid,\n",
    "                               scoring='accuracy',\n",
    "                               cv=cross_validation,\n",
    "                               verbose=4,\n",
    "                               n_jobs=-1,\n",
    "                               refit=True)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest score:\", grid_search.best_score_)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, best_model_pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(y_test, best_model_pred))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best score: 0.49805844756648743\n",
      "Best parameters: {}\n",
      "Best estimator: MultinomialNB()\n",
      "\n",
      "Confusion matrix:\n",
      " [[  86    0  638    1    8]\n",
      " [   2    0  130    0    1]\n",
      " [  34    0 1568    0   19]\n",
      " [   6    0  273    1    2]\n",
      " [  16    0  524    0   40]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.60      0.12      0.20       733\n",
      "      Policy       0.00      0.00      0.00       133\n",
      "       Value       0.50      0.97      0.66      1621\n",
      "    Value(+)       0.50      0.00      0.01       282\n",
      "    Value(-)       0.57      0.07      0.12       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.43      0.23      0.20      3349\n",
      "weighted avg       0.51      0.51      0.38      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "mnb = grid_search(clf, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best score: 0.49223484959359454\n",
      "Best parameters: {}\n",
      "Best estimator: ComplementNB()\n",
      "\n",
      "Confusion matrix:\n",
      " [[291  30 285  49  78]\n",
      " [  6  38  74   9   6]\n",
      " [266  72 968 116 199]\n",
      " [ 41   8 123  89  21]\n",
      " [ 61  30 198  20 271]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.40      0.42       733\n",
      "      Policy       0.21      0.29      0.24       133\n",
      "       Value       0.59      0.60      0.59      1621\n",
      "    Value(+)       0.31      0.32      0.32       282\n",
      "    Value(-)       0.47      0.47      0.47       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.40      0.41      0.41      3349\n",
      "weighted avg       0.50      0.49      0.50      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = ComplementNB()\n",
    "\n",
    "cnb = grid_search(clf, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmna\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1317: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.5164247078694941\n",
      "Best parameters: {'class_weight': None, 'l1_ratio': 0.5, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best estimator: LogisticRegression(l1_ratio=0.5, random_state=0, solver='saga')\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 208    1  483    6   35]\n",
      " [   4   14  112    2    1]\n",
      " [ 162    5 1366    9   79]\n",
      " [  32    3  207   31    9]\n",
      " [  47    2  400    0  131]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.46      0.28      0.35       733\n",
      "      Policy       0.56      0.11      0.18       133\n",
      "       Value       0.53      0.84      0.65      1621\n",
      "    Value(+)       0.65      0.11      0.19       282\n",
      "    Value(-)       0.51      0.23      0.31       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.54      0.31      0.34      3349\n",
      "weighted avg       0.52      0.52      0.47      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "parameter_grid= {'solver': ['saga'], 'penalty': ['elasticnet', 'l1', 'l2'], 'l1_ratio': [0.5], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "lg = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Best score: 0.5203067255318313\n",
      "Best parameters: {'class_weight': None, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "Best estimator: SGDClassifier(n_jobs=-1, random_state=0)\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 218    2  429   25   59]\n",
      " [   4   37   82    7    3]\n",
      " [ 175   16 1252   34  144]\n",
      " [  36    4  166   53   23]\n",
      " [  49    4  319   10  198]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.45      0.30      0.36       733\n",
      "      Policy       0.59      0.28      0.38       133\n",
      "       Value       0.56      0.77      0.65      1621\n",
      "    Value(+)       0.41      0.19      0.26       282\n",
      "    Value(-)       0.46      0.34      0.39       580\n",
      "\n",
      "    accuracy                           0.52      3349\n",
      "   macro avg       0.49      0.38      0.41      3349\n",
      "weighted avg       0.51      0.52      0.50      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# The ‘log’ loss gives logistic regression, ‘perceptron’ is the linear loss used by the perceptron algorithm\n",
    "parameter_grid= {'loss': ['hinge', 'perceptron'], 'penalty': ['elasticnet', 'l1', 'l2'], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "sgd = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "parameter_grid= {}\n",
    "\n",
    "dt = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "parameter_grid= {}\n",
    "\n",
    "rf = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(random_state=0)\n",
    "\n",
    "parameter_grid= {}\n",
    "\n",
    "svc = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(random_state=0)\n",
    "\n",
    "parameter_grid= {}\n",
    "\n",
    "per = grid_search(clf, parameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(random_state=0)\n",
    "\n",
    "parameter_grid= {}\n",
    "\n",
    "xgb = grid_search(clf, parameter_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
