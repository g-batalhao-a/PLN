{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stopwords and Stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "\n",
    "# Remover algumas palavras da lista, p.ex. \"não\" para manter a negação\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Esta unidade curricular de Processamento de Linguagem Natural é extremamente divertida e útil porque nos ajuda a desenvolver novas competências no campo da Aprendizagem Computacional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Esta unidade curricular Processamento Linguagem Natural extremamente divertida útil porque ajuda desenvolver novas competências campo Aprendizagem Computacional'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in text.split() if w not in stopwords_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying some Portuguese Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      " esta unidad curricular de processamento de linguagem natur é extremament divertida e útil porqu no ajuda a desenvolv nova competência no campo da aprendizagem computacion\n",
      "\n",
      "Snowball Portugue Stemmer:\n",
      " esta unidad curricul de process de linguag natural é extrem divert e útil porqu nos ajud a desenvolv nov competent no camp da aprendizag computacional\n",
      "\n",
      "RSLP Stemmer:\n",
      " est unidad curricul de process de lingu natur é extrem divert e útil porqu no ajud a desenvolv nov compet no camp da aprendiz computac\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, RSLPStemmer\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "snb = PortugueseStemmer()\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "text_list = text.split()\n",
    "\n",
    "print(\"Porter Stemmer:\\n\", ' '.join([ps.stem(w) for w in text_list]))\n",
    "print(\"\\nSnowball Portugue Stemmer:\\n\", ' '.join([snb.stem(w) for w in text_list]))\n",
    "print(\"\\nRSLP Stemmer:\\n\",' '.join([rslp.stem(w) for w in text_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_excel('OpArticles_ADUs.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base code to test the different Stemmers with a TfidfVectorizer and ComplementNB Classifier, later compare their metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "y = dataset['label']\n",
    "\n",
    "def test_stemmer(stemmer):\n",
    "    corpus = []\n",
    "\n",
    "    # Preprocess\n",
    "    start = time.time()\n",
    "    for i in range(0, dataset['tokens'].size):\n",
    "        # get review, remove and lowercase non alpha chars\n",
    "        review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', dataset['tokens'][i]).lower()\n",
    "        # split into tokens, apply stemming and remove stop words\n",
    "        review = ' '.join([stemmer.stem(w) for w in review.split() if w not in stopwords_list])\n",
    "        corpus.append(review)\n",
    "    stop = time.time()\n",
    "    print(\"Preprocess time: %0.2fs\" % (stop - start))\n",
    "\n",
    "    print(corpus[:5])\n",
    "\n",
    "    # Fit vectorizer\n",
    "    X = vectorizer.fit_transform(corpus).toarray()\n",
    "    print(\"\\n(Number of samples, Number of features):\", X.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "    start = time.time()\n",
    "    clf = ComplementNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    stop = time.time()\n",
    "\n",
    "    print(\"\\nModel time: %0.2fs\" % (stop - start))\n",
    "    print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 4.05s\n",
      "['facto não apena fruto ignorância', 'havia humor jornalismo investigação preocupação aprofundar contextualizar história isenção relato preocupação social urgência denunciar muita peça realment jornalística', 'tudo cómico fifa', 'todo permitimo organização faça totalment absurdo sentido', 'não fazem rir custa poderoso']\n",
      "\n",
      "(Number of samples, Number of features): (16743, 15170)\n",
      "\n",
      "Model time: 1.22s\n",
      "\n",
      "Confusion matrix:\n",
      " [[306  22 277  44  84]\n",
      " [  4  57  59   8   5]\n",
      " [278  71 966  99 207]\n",
      " [ 43  22 105  93  19]\n",
      " [ 70  33 187  20 270]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.42      0.43       733\n",
      "      Policy       0.28      0.43      0.34       133\n",
      "       Value       0.61      0.60      0.60      1621\n",
      "    Value(+)       0.35      0.33      0.34       282\n",
      "    Value(-)       0.46      0.47      0.46       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.43      0.45      0.43      3349\n",
      "weighted avg       0.51      0.51      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stemmer(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portuguese Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 4.87s\n",
      "['fact nã apen frut ignor', 'hav humor jornal investig preocup aprofund contextualiz histór isençã relat preocup social urgênc denunc muit pec realment jornalíst', 'tud cómic fif', 'tod permit organiz fac total absurd sent', 'nã faz rir cust poder']\n",
      "\n",
      "(Number of samples, Number of features): (16743, 9148)\n",
      "\n",
      "Model time: 0.62s\n",
      "\n",
      "Confusion matrix:\n",
      " [[304  29 287  42  71]\n",
      " [  9  38  71   8   7]\n",
      " [260  73 981 108 199]\n",
      " [ 40  12 117  90  23]\n",
      " [ 66  25 189  20 280]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.45      0.41      0.43       733\n",
      "      Policy       0.21      0.29      0.25       133\n",
      "       Value       0.60      0.61      0.60      1621\n",
      "    Value(+)       0.34      0.32      0.33       282\n",
      "    Value(-)       0.48      0.48      0.48       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.42      0.42      0.42      3349\n",
      "weighted avg       0.51      0.51      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stemmer(snb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 7.28s\n",
      "['fact não apen frut ignor', 'hav hum jorn investig preocup aprofund contextual histór isenç relat preocup soc urg denunci muit peç real jorn', 'tud cómic fif', 'tod permit organiz faç total absurd sent', 'não faz rir cust poder']\n",
      "\n",
      "(Number of samples, Number of features): (16743, 8256)\n",
      "\n",
      "Model time: 0.39s\n",
      "\n",
      "Confusion matrix:\n",
      " [[300  28 288  42  75]\n",
      " [  6  41  70   9   7]\n",
      " [263  66 968 112 212]\n",
      " [ 42  11 115  93  21]\n",
      " [ 69  28 196  18 269]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.41      0.42       733\n",
      "      Policy       0.24      0.31      0.27       133\n",
      "       Value       0.59      0.60      0.59      1621\n",
      "    Value(+)       0.34      0.33      0.33       282\n",
      "    Value(-)       0.46      0.46      0.46       580\n",
      "\n",
      "    accuracy                           0.50      3349\n",
      "   macro avg       0.41      0.42      0.42      3349\n",
      "weighted avg       0.50      0.50      0.50      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stemmer(rslp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Porter Stemmer_  had similar metrics to the others, but it does not make sense to use it since it is not a Stemmer aimed to Portuguese text, was only used as a matter of comparison.\n",
    "\n",
    "We will use the _RSLP Stemmer_ as it trims more the word endings and leads to less features, causing smaller matrixes for the classifiers and thus faster times and less resources.\n",
    "Even though the preprocess time was double compared to the _Snowball Portuguese Stemmer_, the metrics are simliar, and it is only run once in the preprocess before the classification tasks, so the advantages outweight this disavantage."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55f82424ee0038bf843e9f84700baa6bcc97076ded2739a9002049e8174a9129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
