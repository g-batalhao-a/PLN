{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stopwords and Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Esta unidade curricular de Processamento de Linguagem Natural é extremamente divertida e útil porque nos ajuda a desenvolver novas competências no campo da Aprendizagem Computacional não muito bem lecionada pelo Carlos Sexy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portuguese Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "\n",
    "# Remover algumas palavras da lista, p.ex. \"não\"\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Esta unidade curricular Processamento Linguagem Natural extremamente divertida útil porque ajuda desenvolver novas competências campo Aprendizagem Computacional não bem lecionada Carlos Sexy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in text.split() if w not in stopwords_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying some Portuguese Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      " esta unidad curricular de processamento de linguagem natur é extremament divertida e útil porqu no ajuda a desenvolv nova competência no campo da aprendizagem computacion não muito bem lecionada pelo carlo sexi\n",
      "\n",
      "Snowball Portugue Stemmer:\n",
      " esta unidad curricul de process de linguag natural é extrem divert e útil porqu nos ajud a desenvolv nov competent no camp da aprendizag computacional nã muit bem lecion pel carl sexy\n",
      "\n",
      "RSLP Stemmer:\n",
      " est unidad curricul de process de lingu natur é extrem divert e útil porqu no ajud a desenvolv nov compet no camp da aprendiz computac não muit bem lecion pel carl sexy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, RSLPStemmer\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "snb = PortugueseStemmer()\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "text_list = text.split()\n",
    "\n",
    "print(\"Porter Stemmer:\\n\", ' '.join([ps.stem(w) for w in text_list]))\n",
    "print(\"\\nSnowball Portugue Stemmer:\\n\", ' '.join([snb.stem(w) for w in text_list]))\n",
    "print(\"\\nRSLP Stemmer:\\n\",' '.join([rslp.stem(w) for w in text_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_excel('OpArticles_ADUs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "stopwords_list = stopwords.words('portuguese')\n",
    "stopwords_list.remove('não')\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode')\n",
    "\n",
    "y = dataset['label']\n",
    "\n",
    "def test_stemmer(stemmer):\n",
    "    corpus = []\n",
    "\n",
    "    # Preprocess\n",
    "    start = time.time()\n",
    "    for i in range(0, dataset['tokens'].size):\n",
    "        # get review, remove and lowercase non alpha chars\n",
    "        review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', dataset['tokens'][i]).lower()\n",
    "        # split into tokens, apply stemming and remove stop words\n",
    "        review = ' '.join([stemmer.stem(w) for w in review.split() if w not in stopwords_list])\n",
    "        corpus.append(review)\n",
    "    stop = time.time()\n",
    "    print(\"Preprocess time: %0.2fs\" % (stop - start))\n",
    "\n",
    "    # Fit vectorizer\n",
    "    X = vectorizer.fit_transform(corpus).toarray()\n",
    "    print(\"\\n(Number of samples, Number of features):\", X.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0, stratify=y, shuffle=True)\n",
    "\n",
    "    start = time.time()\n",
    "    clf = ComplementNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    stop = time.time()\n",
    "\n",
    "    print(\"\\nModel time: %0.2fs\" % (stop - start))\n",
    "    print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 2.73s\n",
      "\n",
      "(Number of samples, Number of features): (16743, 15018)\n",
      "\n",
      "Model time: 0.63s\n",
      "\n",
      "Confusion matrix:\n",
      " [[306  21 279  45  82]\n",
      " [  3  55  61   8   6]\n",
      " [277  73 964  97 210]\n",
      " [ 41  22 108  92  19]\n",
      " [ 72  32 184  22 270]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.42      0.43       733\n",
      "      Policy       0.27      0.41      0.33       133\n",
      "       Value       0.60      0.59      0.60      1621\n",
      "    Value(+)       0.35      0.33      0.34       282\n",
      "    Value(-)       0.46      0.47      0.46       580\n",
      "\n",
      "    accuracy                           0.50      3349\n",
      "   macro avg       0.42      0.44      0.43      3349\n",
      "weighted avg       0.51      0.50      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stemmer(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portuguese Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 3.16s\n",
      "\n",
      "(Number of samples, Number of features): (16743, 8773)\n",
      "\n",
      "Model time: 0.34s\n",
      "\n",
      "Confusion matrix:\n",
      " [[300  30 292  40  71]\n",
      " [  8  41  68   8   8]\n",
      " [272  68 974 103 204]\n",
      " [ 42  12 113  94  21]\n",
      " [ 64  21 191  17 287]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.41      0.42       733\n",
      "      Policy       0.24      0.31      0.27       133\n",
      "       Value       0.59      0.60      0.60      1621\n",
      "    Value(+)       0.36      0.33      0.35       282\n",
      "    Value(-)       0.49      0.49      0.49       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.42      0.43      0.43      3349\n",
      "weighted avg       0.51      0.51      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stemmer(snb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 5.25s\n",
      "\n",
      "(Number of samples, Number of features): (16743, 7843)\n",
      "\n",
      "Model time: 0.30s\n",
      "\n",
      "Confusion matrix:\n",
      " [[298  23 292  46  74]\n",
      " [  8  38  70  10   7]\n",
      " [270  65 954 112 220]\n",
      " [ 38  12 117  95  20]\n",
      " [ 69  29 190  20 272]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.44      0.41      0.42       733\n",
      "      Policy       0.23      0.29      0.25       133\n",
      "       Value       0.59      0.59      0.59      1621\n",
      "    Value(+)       0.34      0.34      0.34       282\n",
      "    Value(-)       0.46      0.47      0.46       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.41      0.42      0.41      3349\n",
      "weighted avg       0.50      0.49      0.50      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_stemmer(rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55f82424ee0038bf843e9f84700baa6bcc97076ded2739a9002049e8174a9129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
