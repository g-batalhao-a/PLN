{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in Gensim\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) is a model for training word embeddings that revolutionized the way words are represented. [Gensim](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html) provides an implementation of the algorithm, with which we can train our own word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_excel(\"OpArticles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "documents = []\n",
    "for i in range(0, articles['body'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', articles['body'][i]).lower()\n",
    "    # add review to corpus\n",
    "    documents.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "model_articles = Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=10, sg=1)\n",
    "\n",
    "print(\"Training time:\", datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles.wv.save(\"./word_vectors/articles_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles = KeyedVectors.load(\"./word_vectors/articles_wv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese embeddings\n",
    "\n",
    "A number of embeddings for Portuguese are available at [NILC](http://nilc.icmc.usp.br/embeddings), as well as at the [NLX-group](https://github.com/nlx-group/LX-DSemVectors).\n",
    "\n",
    "Using FastText skip-gram 1000 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while to load...\n",
    "model_pt = KeyedVectors.load_word2vec_format('./word_vectors/skip_s1000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word vectors\n",
    "model_pt.save(\"./word_vectors/pt_wv_s1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model word vectors (much faster than the above)\n",
    "model_pt = KeyedVectors.load(\"./word_vectors/pt_wv_s1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('OpArticles_ADUs.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, dataset['tokens'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', dataset['tokens'][i]).lower()\n",
    "    # add review to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the length of the input\n",
    "\n",
    "The reviews in our corpus have variable length. However, we need to represent them with a fixed-length vector of features. One way to do it is to impose a limit on the number of word embeddings we want to include.\n",
    "\n",
    "To convert words into their vector representations (embeddings), let's create an auxiliary function that takes in the number of embeddings we wish to include in the representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "    # split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "    \n",
    "    # add blanks up to sequence_len, if needed\n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above *text_to_vector* function takes an *embeddings* dictionary, the *text* to convert, and the number of words *sequence_len* from *text* to consider. It returns a vector with appended embeddings for the first *sequence_len* words that exist in the *embeddings* dictionary (tokens for which no embedding is found are ignored). In case the text has less than *sequence_len* words for which we have embeddings, blank embeddings will be added.\n",
    "\n",
    "To better decide how many word embeddings we wish to append, let's learn a bit more about the length of each review in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 82 14.30406737143881 9.470560303048728 ModeResult(mode=array([8]), count=array([972]))\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 2250) (16743,)\n"
     ]
    }
   ],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model_articles, c, 15))\n",
    "\n",
    "X = np.array(embeddings_corpus)\n",
    "y = dataset['label']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[  96    2  632    0    3]\n",
      " [   2    8  123    0    0]\n",
      " [  65    5 1538    7    6]\n",
      " [  19    0  259    3    1]\n",
      " [  31    1  538    4    6]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.45      0.13      0.20       733\n",
      "      Policy       0.50      0.06      0.11       133\n",
      "       Value       0.50      0.95      0.65      1621\n",
      "    Value(+)       0.21      0.01      0.02       282\n",
      "    Value(-)       0.38      0.01      0.02       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.41      0.23      0.20      3349\n",
      "weighted avg       0.44      0.49      0.37      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "clf = SGDClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NILC PT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 15000) (16743,)\n"
     ]
    }
   ],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model_pt, c, 15))\n",
    "\n",
    "X = np.array(embeddings_corpus)\n",
    "y = dataset['label']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[301   1 293  40  98]\n",
      " [  3  60  47  14   9]\n",
      " [301  19 951  89 261]\n",
      " [ 46   3 105 106  22]\n",
      " [ 74   2 211   9 284]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.42      0.41      0.41       733\n",
      "      Policy       0.71      0.45      0.55       133\n",
      "       Value       0.59      0.59      0.59      1621\n",
      "    Value(+)       0.41      0.38      0.39       282\n",
      "    Value(-)       0.42      0.49      0.45       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.51      0.46      0.48      3349\n",
      "weighted avg       0.51      0.51      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "clf = SGDClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55f82424ee0038bf843e9f84700baa6bcc97076ded2739a9002049e8174a9129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
