{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in Gensim\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) is a model for training word embeddings that revolutionized the way words are represented. [Gensim](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html) provides an implementation of the algorithm, with which we can train our own word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>body</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>topics</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>url_canonical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>Pouco pão e muito circo, morte e bocejo</td>\n",
       "      <td>['José Vítor Malheiros']</td>\n",
       "      <td>O poeta espanhol António Machado escrevia, uns...</td>\n",
       "      <td>É tudo cómico na FIFA, porque todos os dias a ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Desporto', ...</td>\n",
       "      <td>2014-06-17 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/06/17/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a3fc896a7fea069f0717</td>\n",
       "      <td>Portugal nos Mundiais de Futebol de 2010 e 2014</td>\n",
       "      <td>['Rui J. Baptista']</td>\n",
       "      <td>“O mais excelente quadro posto a uma luz logo ...</td>\n",
       "      <td>Deve ser evidenciado o clima favorável criado ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Brasil', 'Campeonato do Mundo', 'Coreia do N...</td>\n",
       "      <td>2014-07-05 02:46:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/05/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a455896a7fea069f07ab</td>\n",
       "      <td>Futebol, guerra, religião</td>\n",
       "      <td>['Fernando Belo']</td>\n",
       "      <td>1. As sociedades humanas parecem ser regidas p...</td>\n",
       "      <td>O futebol parece ser um sucedâneo quer da lei ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['A guerra na Síria', 'Desporto', 'Futebol', '...</td>\n",
       "      <td>2014-07-12 16:05:33</td>\n",
       "      <td>https://www.publico.pt/2014/07/12/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a52f896a7fea069f0921</td>\n",
       "      <td>As razões do Qatar para acolher o Mundial em 2022</td>\n",
       "      <td>['Hamad bin Khalifa bin Ahmad Al Thani']</td>\n",
       "      <td>Este foi um Mundial incrível. Vimos actuações ...</td>\n",
       "      <td>Queremos cooperar plenamente com a investigaçã...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...</td>\n",
       "      <td>2014-07-27 02:00:00</td>\n",
       "      <td>https://www.publico.pt/2014/07/27/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a8d7896a7fea069f6997</td>\n",
       "      <td>A política no campo de futebol</td>\n",
       "      <td>['Carlos Nolasco']</td>\n",
       "      <td>O futebol sempre foi um jogo aparentemente sim...</td>\n",
       "      <td>Retirar a expressão política do futebol é reti...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Albânia', 'Campeonato da Europa', 'Desporto'...</td>\n",
       "      <td>2014-10-23 00:16:00</td>\n",
       "      <td>https://www.publico.pt/2014/10/23/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>5cee2df3896a7fea06c54a35</td>\n",
       "      <td>Jogador residente, o aplauso de uma raridade</td>\n",
       "      <td>['Nuno Sousa']</td>\n",
       "      <td>Era apenas mais um jogo da Lazio, em final de ...</td>\n",
       "      <td>Era apenas mais um jogo da Lazio, em final de ...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>['Desporto', 'Futebol', 'Futebol internacional...</td>\n",
       "      <td>2019-05-29 07:32:00</td>\n",
       "      <td>https://www.publico.pt/2019/05/29/desporto/opi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>5ceee4c4896a7fea06cc3895</td>\n",
       "      <td>“Brexit”: uma opinião estática?</td>\n",
       "      <td>['Francisco Bethencourt']</td>\n",
       "      <td>As eleições europeias no Reino Unido estão a s...</td>\n",
       "      <td>O problema da participação na União Europeia e...</td>\n",
       "      <td>World</td>\n",
       "      <td>['Brexit', 'Eleições europeias', 'Mundo', 'Opi...</td>\n",
       "      <td>2019-05-29 20:00:00</td>\n",
       "      <td>https://www.publico.pt/2019/05/29/mundo/opinia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>5cef7f74896a7fea06d223f7</td>\n",
       "      <td>Socorro, querem roubar-nos a língua e deixar-n...</td>\n",
       "      <td>['Nuno Pacheco']</td>\n",
       "      <td>Estava eu no Brasil, de férias, entretido (e d...</td>\n",
       "      <td>As variantes do português, riquíssimas, merece...</td>\n",
       "      <td>Culture</td>\n",
       "      <td>['Acordo Ortográfico', 'Brasil', 'CPLP', 'Cult...</td>\n",
       "      <td>2019-05-30 07:30:00</td>\n",
       "      <td>https://www.publico.pt/2019/05/30/culturaipsil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>5cefd3d4896a7fea06d57241</td>\n",
       "      <td>300 dias à espera da cannabis</td>\n",
       "      <td>['Bruno Maia']</td>\n",
       "      <td>Passaram mais de 300 dias desde que a Assemble...</td>\n",
       "      <td>Não existem hoje dúvidas na comunidade científ...</td>\n",
       "      <td>Society</td>\n",
       "      <td>['Cannabis', 'Drogas', 'Medicamentos', 'Opiniã...</td>\n",
       "      <td>2019-05-30 13:18:44</td>\n",
       "      <td>https://www.publico.pt/2019/05/30/sociedade/op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>A escolha do curso superior: Existe transparên...</td>\n",
       "      <td>['Tiago Neves Sequeira']</td>\n",
       "      <td>O mês de julho é o mês da primeira fase de can...</td>\n",
       "      <td>O mês de julho é o mês da primeira fase de can...</td>\n",
       "      <td>Society</td>\n",
       "      <td>['Alunos', 'Educação', 'Ensino Superior', 'Fam...</td>\n",
       "      <td>2019-06-03 06:44:00</td>\n",
       "      <td>https://www.publico.pt/2019/06/03/sociedade/op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article_id  \\\n",
       "0    5d04a31b896a7fea069ef06f   \n",
       "1    5d04a3fc896a7fea069f0717   \n",
       "2    5d04a455896a7fea069f07ab   \n",
       "3    5d04a52f896a7fea069f0921   \n",
       "4    5d04a8d7896a7fea069f6997   \n",
       "..                        ...   \n",
       "368  5cee2df3896a7fea06c54a35   \n",
       "369  5ceee4c4896a7fea06cc3895   \n",
       "370  5cef7f74896a7fea06d223f7   \n",
       "371  5cefd3d4896a7fea06d57241   \n",
       "372  5cf4b764896a7fea06032673   \n",
       "\n",
       "                                                 title  \\\n",
       "0              Pouco pão e muito circo, morte e bocejo   \n",
       "1      Portugal nos Mundiais de Futebol de 2010 e 2014   \n",
       "2                            Futebol, guerra, religião   \n",
       "3    As razões do Qatar para acolher o Mundial em 2022   \n",
       "4                       A política no campo de futebol   \n",
       "..                                                 ...   \n",
       "368       Jogador residente, o aplauso de uma raridade   \n",
       "369                    “Brexit”: uma opinião estática?   \n",
       "370  Socorro, querem roubar-nos a língua e deixar-n...   \n",
       "371                      300 dias à espera da cannabis   \n",
       "372  A escolha do curso superior: Existe transparên...   \n",
       "\n",
       "                                      authors  \\\n",
       "0                    ['José Vítor Malheiros']   \n",
       "1                         ['Rui J. Baptista']   \n",
       "2                           ['Fernando Belo']   \n",
       "3    ['Hamad bin Khalifa bin Ahmad Al Thani']   \n",
       "4                          ['Carlos Nolasco']   \n",
       "..                                        ...   \n",
       "368                            ['Nuno Sousa']   \n",
       "369                 ['Francisco Bethencourt']   \n",
       "370                          ['Nuno Pacheco']   \n",
       "371                            ['Bruno Maia']   \n",
       "372                  ['Tiago Neves Sequeira']   \n",
       "\n",
       "                                                  body  \\\n",
       "0    O poeta espanhol António Machado escrevia, uns...   \n",
       "1    “O mais excelente quadro posto a uma luz logo ...   \n",
       "2    1. As sociedades humanas parecem ser regidas p...   \n",
       "3    Este foi um Mundial incrível. Vimos actuações ...   \n",
       "4    O futebol sempre foi um jogo aparentemente sim...   \n",
       "..                                                 ...   \n",
       "368  Era apenas mais um jogo da Lazio, em final de ...   \n",
       "369  As eleições europeias no Reino Unido estão a s...   \n",
       "370  Estava eu no Brasil, de férias, entretido (e d...   \n",
       "371  Passaram mais de 300 dias desde que a Assemble...   \n",
       "372  O mês de julho é o mês da primeira fase de can...   \n",
       "\n",
       "                                      meta_description   topics  \\\n",
       "0    É tudo cómico na FIFA, porque todos os dias a ...   Sports   \n",
       "1    Deve ser evidenciado o clima favorável criado ...   Sports   \n",
       "2    O futebol parece ser um sucedâneo quer da lei ...   Sports   \n",
       "3    Queremos cooperar plenamente com a investigaçã...   Sports   \n",
       "4    Retirar a expressão política do futebol é reti...   Sports   \n",
       "..                                                 ...      ...   \n",
       "368  Era apenas mais um jogo da Lazio, em final de ...   Sports   \n",
       "369  O problema da participação na União Europeia e...    World   \n",
       "370  As variantes do português, riquíssimas, merece...  Culture   \n",
       "371  Não existem hoje dúvidas na comunidade científ...  Society   \n",
       "372  O mês de julho é o mês da primeira fase de can...  Society   \n",
       "\n",
       "                                              keywords         publish_date  \\\n",
       "0    ['Brasil', 'Campeonato do Mundo', 'Desporto', ...  2014-06-17 00:16:00   \n",
       "1    ['Brasil', 'Campeonato do Mundo', 'Coreia do N...  2014-07-05 02:46:00   \n",
       "2    ['A guerra na Síria', 'Desporto', 'Futebol', '...  2014-07-12 16:05:33   \n",
       "3    ['Desporto', 'FIFA', 'Futebol', 'Mundial de fu...  2014-07-27 02:00:00   \n",
       "4    ['Albânia', 'Campeonato da Europa', 'Desporto'...  2014-10-23 00:16:00   \n",
       "..                                                 ...                  ...   \n",
       "368  ['Desporto', 'Futebol', 'Futebol internacional...  2019-05-29 07:32:00   \n",
       "369  ['Brexit', 'Eleições europeias', 'Mundo', 'Opi...  2019-05-29 20:00:00   \n",
       "370  ['Acordo Ortográfico', 'Brasil', 'CPLP', 'Cult...  2019-05-30 07:30:00   \n",
       "371  ['Cannabis', 'Drogas', 'Medicamentos', 'Opiniã...  2019-05-30 13:18:44   \n",
       "372  ['Alunos', 'Educação', 'Ensino Superior', 'Fam...  2019-06-03 06:44:00   \n",
       "\n",
       "                                         url_canonical  \n",
       "0    https://www.publico.pt/2014/06/17/desporto/opi...  \n",
       "1    https://www.publico.pt/2014/07/05/desporto/opi...  \n",
       "2    https://www.publico.pt/2014/07/12/desporto/opi...  \n",
       "3    https://www.publico.pt/2014/07/27/desporto/opi...  \n",
       "4    https://www.publico.pt/2014/10/23/desporto/opi...  \n",
       "..                                                 ...  \n",
       "368  https://www.publico.pt/2019/05/29/desporto/opi...  \n",
       "369  https://www.publico.pt/2019/05/29/mundo/opinia...  \n",
       "370  https://www.publico.pt/2019/05/30/culturaipsil...  \n",
       "371  https://www.publico.pt/2019/05/30/sociedade/op...  \n",
       "372  https://www.publico.pt/2019/06/03/sociedade/op...  \n",
       "\n",
       "[373 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_excel(\"OpArticles.xlsx\")\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "documents = []\n",
    "for i in range(0, articles['body'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', articles['body'][i]).lower()\n",
    "    # add review to corpus\n",
    "    documents.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:00:03.988001\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "model_articles = Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=10, sg=1)\n",
    "\n",
    "print(\"Training time:\", datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles.wv.save(\"./word_vectors/articles_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles = KeyedVectors.load(\"./word_vectors/articles_wv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese embeddings\n",
    "\n",
    "A number of embeddings for Portuguese are available at [NILC](http://nilc.icmc.usp.br/embeddings), as well as at the [NLX-group](https://github.com/nlx-group/LX-DSemVectors).\n",
    "\n",
    "Using FastText skip-gram 1000 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while to load...\n",
    "model_pt = KeyedVectors.load_word2vec_format('./word_vectors/skip_s1000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word vectors\n",
    "model_pt.save(\"./word_vectors/pt_wv_s1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model word vectors (much faster than the above)\n",
    "model_pt = KeyedVectors.load(\"./word_vectors/pt_wv_s1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>node</th>\n",
       "      <th>ranges</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2516, 2556]]</td>\n",
       "      <td>O facto não é apenas fruto da ignorância</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[[2568, 2806]]</td>\n",
       "      <td>havia no seu humor mais jornalismo (mais inves...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[[3169, 3190]]</td>\n",
       "      <td>É tudo cómico na FIFA</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[[3198, 3285]]</td>\n",
       "      <td>o que todos nós permitimos que esta organizaçã...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4257, 4296]]</td>\n",
       "      <td>não nos fazem rir à custa dos poderosos</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16738</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>29</td>\n",
       "      <td>[[4980, 5041], [5074, 5279]]</td>\n",
       "      <td>A única variável disponibilizada que pode ser ...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16739</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>30</td>\n",
       "      <td>[[5293, 5340]]</td>\n",
       "      <td>esse número esconde informação muito pertinente</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16740</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>32</td>\n",
       "      <td>[[5053, 5072]]</td>\n",
       "      <td>bastante imperfeita</td>\n",
       "      <td>Value(-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16741</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>34</td>\n",
       "      <td>[[5549, 5643]]</td>\n",
       "      <td>esconde também a proporção de diplomados que e...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16742</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>35</td>\n",
       "      <td>[[5488, 5537]]</td>\n",
       "      <td>esconde a distribuição de salários dos diplomados</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     article_id annotator  node                        ranges  \\\n",
       "0      5d04a31b896a7fea069ef06f         A     0                [[2516, 2556]]   \n",
       "1      5d04a31b896a7fea069ef06f         A     1                [[2568, 2806]]   \n",
       "2      5d04a31b896a7fea069ef06f         A     3                [[3169, 3190]]   \n",
       "3      5d04a31b896a7fea069ef06f         A     4                [[3198, 3285]]   \n",
       "4      5d04a31b896a7fea069ef06f         A     6                [[4257, 4296]]   \n",
       "...                         ...       ...   ...                           ...   \n",
       "16738  5cf4b764896a7fea06032673         D    29  [[4980, 5041], [5074, 5279]]   \n",
       "16739  5cf4b764896a7fea06032673         D    30                [[5293, 5340]]   \n",
       "16740  5cf4b764896a7fea06032673         D    32                [[5053, 5072]]   \n",
       "16741  5cf4b764896a7fea06032673         D    34                [[5549, 5643]]   \n",
       "16742  5cf4b764896a7fea06032673         D    35                [[5488, 5537]]   \n",
       "\n",
       "                                                  tokens     label  \n",
       "0               O facto não é apenas fruto da ignorância     Value  \n",
       "1      havia no seu humor mais jornalismo (mais inves...     Value  \n",
       "2                                  É tudo cómico na FIFA     Value  \n",
       "3      o que todos nós permitimos que esta organizaçã...     Value  \n",
       "4                não nos fazem rir à custa dos poderosos     Value  \n",
       "...                                                  ...       ...  \n",
       "16738  A única variável disponibilizada que pode ser ...     Value  \n",
       "16739    esse número esconde informação muito pertinente      Fact  \n",
       "16740                                bastante imperfeita  Value(-)  \n",
       "16741  esconde também a proporção de diplomados que e...     Value  \n",
       "16742  esconde a distribuição de salários dos diplomados      Fact  \n",
       "\n",
       "[16743 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_excel('OpArticles_ADUs.xlsx')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, dataset['tokens'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', dataset['tokens'][i]).lower()\n",
    "    # add review to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the length of the input\n",
    "\n",
    "The reviews in our corpus have variable length. However, we need to represent them with a fixed-length vector of features. One way to do it is to impose a limit on the number of word embeddings we want to include.\n",
    "\n",
    "To convert words into their vector representations (embeddings), let's create an auxiliary function that takes in the number of embeddings we wish to include in the representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "    # split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "    \n",
    "    # add blanks up to sequence_len, if needed\n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above *text_to_vector* function takes an *embeddings* dictionary, the *text* to convert, and the number of words *sequence_len* from *text* to consider. It returns a vector with appended embeddings for the first *sequence_len* words that exist in the *embeddings* dictionary (tokens for which no embedding is found are ignored). In case the text has less than *sequence_len* words for which we have embeddings, blank embeddings will be added.\n",
    "\n",
    "To better decide how many word embeddings we wish to append, let's learn a bit more about the length of each review in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 82 14.30406737143881 9.470560303048728 ModeResult(mode=array([8]), count=array([972]))\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 2250) (16743,)\n"
     ]
    }
   ],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model_articles, c, 15))\n",
    "\n",
    "X = np.array(embeddings_corpus)\n",
    "y = dataset['label']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[ 220   59  392   20   42]\n",
      " [   9   53   70    0    1]\n",
      " [ 200  123 1187   33   78]\n",
      " [  45   33  173   12   19]\n",
      " [  92   27  404   10   47]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.39      0.30      0.34       733\n",
      "      Policy       0.18      0.40      0.25       133\n",
      "       Value       0.53      0.73      0.62      1621\n",
      "    Value(+)       0.16      0.04      0.07       282\n",
      "    Value(-)       0.25      0.08      0.12       580\n",
      "\n",
      "    accuracy                           0.45      3349\n",
      "   macro avg       0.30      0.31      0.28      3349\n",
      "weighted avg       0.41      0.45      0.41      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, stratify=y)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NILC PT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 15000) (16743,)\n"
     ]
    }
   ],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model_pt, c, 15))\n",
    "\n",
    "X = np.array(embeddings_corpus)\n",
    "y = dataset['label']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[273   3 321  42  94]\n",
      " [  3  60  48  13   9]\n",
      " [267  24 999 106 225]\n",
      " [ 41   3 114 101  23]\n",
      " [ 67   3 225  11 274]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.42      0.37      0.39       733\n",
      "      Policy       0.65      0.45      0.53       133\n",
      "       Value       0.59      0.62      0.60      1621\n",
      "    Value(+)       0.37      0.36      0.36       282\n",
      "    Value(-)       0.44      0.47      0.45       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.49      0.45      0.47      3349\n",
      "weighted avg       0.51      0.51      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, stratify=y)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55f82424ee0038bf843e9f84700baa6bcc97076ded2739a9002049e8174a9129"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
