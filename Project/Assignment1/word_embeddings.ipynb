{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Apply functions from the practical classes notebooks, in order to compare the results with the baseline classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in Gensim\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) is a model for training word embeddings that revolutionized the way words are represented. [Gensim](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html) provides an implementation of the algorithm, with which we can train our own word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article's body\n",
    "\n",
    "Our first task is to create a model based on the article's body, hoping that it is easier to predict the labels on a corpus with the same thematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_excel(\"OpArticles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "documents = []\n",
    "for i in range(0, articles['body'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', articles['body'][i]).lower()\n",
    "    # add review to corpus\n",
    "    documents.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.9753782749176025\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "model_articles = Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=10, sg=1)\n",
    "\n",
    "print(\"Training time:\", time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles.wv.save(\"./word_vectors/wv_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles = KeyedVectors.load(\"./word_vectors/wv_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese embeddings\n",
    "\n",
    "A number of embeddings for Portuguese are available at [NILC](http://nilc.icmc.usp.br/embeddings), as well as at the [NLX-group](https://github.com/nlx-group/LX-DSemVectors).\n",
    "\n",
    "We will be testing the following models from NILC\n",
    "- Word2Vec CBOW 100 dimensions\n",
    "- Word2Vec SKIP-GRAM 100 dimensions\n",
    "- FastText CBOW 100 dimensions\n",
    "- FastText SKIP-GRAM 100 dimensions\n",
    "- FastText SKIP-GRAM 1000 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only need to load from .txt once, then load only created word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while to load...\n",
    "model_w2v_cbow_s100 = KeyedVectors.load_word2vec_format('./word_vectors/w2v_cbow_s100.txt')\n",
    "# save model word vectors\n",
    "model_w2v_cbow_s100.save(\"./word_vectors/wv_w2v_cbow_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_skip_s100 = KeyedVectors.load_word2vec_format('./word_vectors/w2v_skip_s100.txt')\n",
    "model_w2v_skip_s100.save(\"./word_vectors/wv_w2v_skip_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_cbow_s100 = KeyedVectors.load_word2vec_format('./word_vectors/ft_cbow_s100.txt')\n",
    "model_ft_cbow_s100.save(\"./word_vectors/wv_ft_cbow_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_skip_s100 = KeyedVectors.load_word2vec_format('./word_vectors/ft_skip_s100.txt')\n",
    "model_ft_skip_s100.save(\"./word_vectors/wv_ft_skip_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_skip_s1000 = KeyedVectors.load_word2vec_format('./word_vectors/ft_skip_s1000.txt')\n",
    "model_ft_skip_s1000.save(\"./word_vectors/wv_ft_skip_s1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model word vectors (much faster than the above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_cbow_s100 = KeyedVectors.load(\"./word_vectors/wv_w2v_cbow_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_skip_s100 = KeyedVectors.load(\"./word_vectors/wv_w2v_skip_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_cbow_s100 = KeyedVectors.load(\"./word_vectors/wv_ft_cbow_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_skip_s100 = KeyedVectors.load(\"./word_vectors/wv_ft_skip_s100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_skip_s1000 = KeyedVectors.load(\"./word_vectors/wv_ft_skip_s1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('OpArticles_ADUs.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, dataset['tokens'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z\\u00C0-\\u00ff]', ' ', dataset['tokens'][i]).lower()\n",
    "    # add review to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the length of the input\n",
    "\n",
    "The reviews in our corpus have variable length. However, we need to represent them with a fixed-length vector of features. One way to do it is to impose a limit on the number of word embeddings we want to include.\n",
    "\n",
    "To convert words into their vector representations (embeddings), let's create an auxiliary function that takes in the number of embeddings we wish to include in the representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "    # split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "    \n",
    "    # add blanks up to sequence_len, if needed\n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above *text_to_vector* function takes an *embeddings* dictionary, the *text* to convert, and the number of words *sequence_len* from *text* to consider. It returns a vector with appended embeddings for the first *sequence_len* words that exist in the *embeddings* dictionary (tokens for which no embedding is found are ignored). In case the text has less than *sequence_len* words for which we have embeddings, blank embeddings will be added.\n",
    "\n",
    "To better decide how many word embeddings we wish to append, let's learn a bit more about the length of each review in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 82 14.30406737143881 9.470560303048728 ModeResult(mode=array([8]), count=array([972]))\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    # convert corpus into dataset with appended embeddings representation\n",
    "    embeddings_corpus = []\n",
    "    for c in corpus:\n",
    "        embeddings_corpus.append(text_to_vector(model, c, 15))\n",
    "\n",
    "    X = np.array(embeddings_corpus)\n",
    "    y = dataset['label']\n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "    clf = SGDClassifier(random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article's body model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 2250) (16743,)\n",
      "\n",
      "Confusion matrix:\n",
      " [[  83    3  623    8   16]\n",
      " [   4    5  120    2    2]\n",
      " [  61    2 1514   14   30]\n",
      " [   9    0  252   14    7]\n",
      " [  16    0  530   14   20]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.48      0.11      0.18       733\n",
      "      Policy       0.50      0.04      0.07       133\n",
      "       Value       0.50      0.93      0.65      1621\n",
      "    Value(+)       0.27      0.05      0.08       282\n",
      "    Value(-)       0.27      0.03      0.06       580\n",
      "\n",
      "    accuracy                           0.49      3349\n",
      "   macro avg       0.40      0.23      0.21      3349\n",
      "weighted avg       0.43      0.49      0.38      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NILC PT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 1500) (16743,)\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 257    7  360   33   76]\n",
      " [   9   42   71    4    7]\n",
      " [ 235   38 1114   63  171]\n",
      " [  56    7  154   45   20]\n",
      " [  79   10  316   10  165]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.40      0.35      0.38       733\n",
      "      Policy       0.40      0.32      0.35       133\n",
      "       Value       0.55      0.69      0.61      1621\n",
      "    Value(+)       0.29      0.16      0.21       282\n",
      "    Value(-)       0.38      0.28      0.32       580\n",
      "\n",
      "    accuracy                           0.48      3349\n",
      "   macro avg       0.41      0.36      0.37      3349\n",
      "weighted avg       0.46      0.48      0.47      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model_w2v_cbow_s100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 1500) (16743,)\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 252    6  342   40   93]\n",
      " [   8   46   71    5    3]\n",
      " [ 231   39 1088   84  179]\n",
      " [  47   10  147   54   24]\n",
      " [  74   14  316   18  158]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.41      0.34      0.37       733\n",
      "      Policy       0.40      0.35      0.37       133\n",
      "       Value       0.55      0.67      0.61      1621\n",
      "    Value(+)       0.27      0.19      0.22       282\n",
      "    Value(-)       0.35      0.27      0.30       580\n",
      "\n",
      "    accuracy                           0.48      3349\n",
      "   macro avg       0.40      0.36      0.38      3349\n",
      "weighted avg       0.46      0.48      0.46      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model_w2v_skip_s100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 1500) (16743,)\n",
      "\n",
      "Confusion matrix:\n",
      " [[278  11 209  59 176]\n",
      " [ 15  53  38   8  19]\n",
      " [372  45 663 111 430]\n",
      " [ 54  10  87  77  54]\n",
      " [115  10 158  33 264]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.33      0.38      0.35       733\n",
      "      Policy       0.41      0.40      0.40       133\n",
      "       Value       0.57      0.41      0.48      1621\n",
      "    Value(+)       0.27      0.27      0.27       282\n",
      "    Value(-)       0.28      0.46      0.35       580\n",
      "\n",
      "    accuracy                           0.40      3349\n",
      "   macro avg       0.37      0.38      0.37      3349\n",
      "weighted avg       0.44      0.40      0.41      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model_ft_cbow_s100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 1500) (16743,)\n",
      "\n",
      "Confusion matrix:\n",
      " [[277   1 306  67  82]\n",
      " [ 11  38  68  10   6]\n",
      " [226  23 993 136 243]\n",
      " [ 41   6 121  82  32]\n",
      " [ 65   1 271  20 223]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.45      0.38      0.41       733\n",
      "      Policy       0.55      0.29      0.38       133\n",
      "       Value       0.56      0.61      0.59      1621\n",
      "    Value(+)       0.26      0.29      0.27       282\n",
      "    Value(-)       0.38      0.38      0.38       580\n",
      "\n",
      "    accuracy                           0.48      3349\n",
      "   macro avg       0.44      0.39      0.41      3349\n",
      "weighted avg       0.48      0.48      0.48      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model_ft_skip_s100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 15000) (16743,)\n",
      "\n",
      "Confusion matrix:\n",
      " [[301   1 293  40  98]\n",
      " [  3  60  47  14   9]\n",
      " [301  19 951  89 261]\n",
      " [ 46   3 105 106  22]\n",
      " [ 74   2 211   9 284]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.42      0.41      0.41       733\n",
      "      Policy       0.71      0.45      0.55       133\n",
      "       Value       0.59      0.59      0.59      1621\n",
      "    Value(+)       0.41      0.38      0.39       282\n",
      "    Value(-)       0.42      0.49      0.45       580\n",
      "\n",
      "    accuracy                           0.51      3349\n",
      "   macro avg       0.51      0.46      0.48      3349\n",
      "weighted avg       0.51      0.51      0.51      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(model_ft_skip_s1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55f82424ee0038bf843e9f84700baa6bcc97076ded2739a9002049e8174a9129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
